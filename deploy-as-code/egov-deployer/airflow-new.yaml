airflow
---
# Source: airflow/templates/rbac/airflow-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: airflow
  namespace: egov
  labels:
    app: airflow
---
# Source: airflow/templates/config/secret-config-envs.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-config-envs
  namespace: egov
  labels:
    app: airflow
## we must use `data` rather than `stringData` (see: https://github.com/helm/helm/issues/10010)
data:
  ## ================
  ## Linux Configs
  ## ================
  TZ: "RXRjL1VUQw=="

  ## ================
  ## Database Configs
  ## ================
  ## database host/port
  DATABASE_HOST: "YWlyZmxvdy1wZ2JvdW5jZXIuZWdvdi5zdmMuY2x1c3Rlci5sb2NhbA=="
  DATABASE_PORT: "NjQzMg=="

  ## database configs
  DATABASE_DB: "YWlyZmxvdw=="
  DATABASE_PROPERTIES: ""

  ## bash command which echos the URL encoded value of $DATABASE_USER
  DATABASE_USER_CMD: "ZWNobyAiJHtEQVRBQkFTRV9VU0VSfSIgfCBweXRob24zIC1jICJpbXBvcnQgdXJsbGliLnBhcnNlOyBlbmNvZGVkX3VzZXIgPSB1cmxsaWIucGFyc2UucXVvdGUoaW5wdXQoKSk7IHByaW50KGVuY29kZWRfdXNlciki"

  ## bash command which echos the URL encoded value of $DATABASE_PASSWORD
  DATABASE_PASSWORD_CMD: "ZWNobyAiJHtEQVRBQkFTRV9QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChlbmNvZGVkX3Bhc3MpIg=="

  ## bash command which echos the DB connection string in SQLAlchemy format
  DATABASE_SQLALCHEMY_CMD: "ZWNobyAtbiAicG9zdGdyZXNxbCtwc3ljb3BnMjovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0ke0RBVEFCQVNFX1BST1BFUlRJRVN9Ig=="

  ## bash command which echos the DB connection string in Celery result_backend format
  DATABASE_CELERY_CMD: "ZWNobyAtbiAiZGIrcG9zdGdyZXNxbDovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0ke0RBVEFCQVNFX1BST1BFUlRJRVN9Ig=="

  ## bash command which echos the DB connection string in `psql` cli format
  DATABASE_PSQL_CMD: "ZWNobyAtbiAicG9zdGdyZXNxbDovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0ke0RBVEFCQVNFX1BST1BFUlRJRVN9Ig=="

  ## ================
  ## Redis Configs
  ## ================
  ## connection string components
  REDIS_HOST: "YWlyZmxvdy1yZWRpcy1tYXN0ZXIuZWdvdi5zdmMuY2x1c3Rlci5sb2NhbA=="
  REDIS_PORT: "NjM3OQ=="
  REDIS_DBNUM: "MQ=="

  ## bash command which echos the URL encoded value of $REDIS_PASSWORD
  ## NOTE: if $REDIS_PASSWORD is non-empty, prints `:${REDIS_PASSWORD}@`, else ``
  REDIS_PASSWORD_CMD: "ZWNobyAiJHtSRURJU19QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChmXCI6e2VuY29kZWRfcGFzc31AXCIpIGlmIGxlbihlbmNvZGVkX3Bhc3MpID4gMCBlbHNlIE5vbmUi"

  ## bash command which echos the Redis connection string
  REDIS_CONNECTION_CMD: "ZWNobyAtbiAicmVkaXM6Ly8kKGV2YWwgJFJFRElTX1BBU1NXT1JEX0NNRCkke1JFRElTX0hPU1R9OiR7UkVESVNfUE9SVH0vJHtSRURJU19EQk5VTX0ke1JFRElTX1BST1BFUlRJRVN9Ig=="

  ## ================
  ## Airflow Configs (General)
  ## ================
  AIRFLOW__CORE__DAGS_FOLDER: "L29wdC9haXJmbG93L2RhZ3MvcmVwby9lZ292LW5hdGlvbmFsLWRhc2hib2FyZC1hY2NlbGVyYXRvci9kYWdz"
  AIRFLOW__CORE__EXECUTOR: "Q2VsZXJ5RXhlY3V0b3I="
  AIRFLOW__CORE__FERNET_KEY: "N1Q1MTJVWFNTbUJPa3BXaW1GSElWYjhqSzZsZm1TQXZ4NG1PNkFyZWhuYz0="
  AIRFLOW__WEBSERVER__SECRET_KEY: "VEhJUyBJUyBVTlNBRkUh"
  AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "ODA4MA=="
  AIRFLOW__CELERY__FLOWER_PORT: "NTU1NQ=="
  ## refresh the dags folder at the same frequency as git-sync
  AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "NjA="

  ## ================
  ## Airflow Configs (Database)
  ## ================
  AIRFLOW__CORE__SQL_ALCHEMY_CONN_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX1NRTEFMQ0hFTVlfQ01EIic="
  ## `core.sql_alchemy_conn` moved to `database.sql_alchemy_conn` in airflow 2.3.0
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX1NRTEFMQ0hFTVlfQ01EIic="

  ## ================
  ## Airflow Configs (Triggerer)
  ## ================
  AIRFLOW__TRIGGERER__DEFAULT_CAPACITY: "MTAwMA=="

  ## ================
  ## Airflow Configs (Logging)
  ## ================
  AIRFLOW__LOGGING__BASE_LOG_FOLDER: "L29wdC9haXJmbG93L2xvZ3M="
  AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION: "L29wdC9haXJmbG93L2xvZ3MvZGFnX3Byb2Nlc3Nvcl9tYW5hZ2VyL2RhZ19wcm9jZXNzb3JfbWFuYWdlci5sb2c="
  AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY: "L29wdC9haXJmbG93L2xvZ3Mvc2NoZWR1bGVy"

  ## ================
  ## Airflow Configs (Celery)
  ## ================
  AIRFLOW__CELERY__WORKER_LOG_SERVER_PORT: "ODc5Mw=="
  AIRFLOW__CELERY__BROKER_URL_CMD: "YmFzaCAtYyAnZXZhbCAiJFJFRElTX0NPTk5FQ1RJT05fQ01EIic="
  AIRFLOW__CELERY__RESULT_BACKEND_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX0NFTEVSWV9DTUQiJw=="

  ## ================
  ## Airflow Configs (Kubernetes)
  ## ================

  ## ================
  ## User Configs
  ## ================
  AIRFLOW__CELERY__FLOWER_URL_PREFIX: "L2FpcmZsb3cvZmxvd2Vy"
  AIRFLOW__WEBSERVER__BASE_URL: "aHR0cHM6Ly9xYS5kaWdpdC5vcmcvYWlyZmxvdw=="
---
# Source: airflow/templates/config/secret-webserver-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-webserver-config
  namespace: egov
  labels:
    app: airflow
data:
  webserver_config.py: "ZnJvbSBmbGFza19hcHBidWlsZGVyLnNlY3VyaXR5Lm1hbmFnZXIgaW1wb3J0IEFVVEhfREIKCiMgdXNlIGVtYmVkZGVkIERCIGZvciBhdXRoCkFVVEhfVFlQRSA9IEFVVEhfREI="
---
# Source: airflow/templates/db-migrations/db-migrations-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-db-migrations
  namespace: egov
  labels:
    app: airflow
    component: db-migrations
data:
  db_migrations.py: "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"
---
# Source: airflow/templates/pgbouncer/pgbouncer-secret-certs.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-pgbouncer-certs
  namespace: egov
  labels:
    app: airflow
    component: pgbouncer
data:
  client.key: "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"
  client.crt: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4VENDQWRtZ0F3SUJBZ0lRUFhEazV3M2U5andRYzNwQkNzdnVaakFOQmdrcWhraUc5dzBCQVFzRkFEQVUKTVJJd0VBWURWUVFERXdsc2IyTmhiR2h2YzNRd0hoY05Nakl3T1RFME1UYzBNalF4V2hjTk1qTXdPVEUwTVRjMApNalF4V2pBVU1SSXdFQVlEVlFRREV3bHNiMk5oYkdodmMzUXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRQ2ptcVN3UlBiNVVHcUp3SWhTcTluNC96ejFBWXB1ZGhHakp0YmRzZWl5WmY0NzZVbmoKUFZwa21vRSt4OElWbDVha2lYQTlLY1VwNVhhZU9YS1d2NjFMemN2RXFKaHZncUJyQUFndlRuSVJkdDFMZEJpTQpUMlpRSW9adThmNE8vbXlCejNPdGV3YWlTRFhJRnVYN3EvWFNkYmlTS0Y4SkdjcnlYK3daNW56SU1ZNmI1TXNZCmttNmMrN29yeXlUQ3lRMnV2NjBxeUFrL0hxOEtFVUFkNnV2cEg2c3BPWGMxYnBWRy9LNHdYck1yV2NsRytpaDYKbFE5WUdBYkxlZlVJV3NnZmkxM1M1T3N1UXJDR0JaK3plNGtmQTA0TmFzVTJEdkhFSldaOTNpL3lzdTEyMVl6WgovZzhOTkI2V0hkOU42bVBDZ05vL05WYUxSMU93d1pIWUJxQVZBZ01CQUFHalB6QTlNQTRHQTFVZER3RUIvd1FFCkF3SUZvREFkQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0RBWURWUjBUQVFIL0JBSXcKQURBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQVNoajBIVjE3QkxUL2hobnpZZmlDa3pWM3dHTHd1dGZaaXdvbApjWWdwOTlrV3JWcW8zNUlka2lydDlQOS9pR2wxR2lPRzduY1NDYmhSWEZyS2lHRlljaElYOEg2VmlzNWFYTURICkFyQSt5WE5VYmk1cTVLZm1kSWp0eFJlSHdsY2wrOEFkMUdLQ1lkZ1hORE5TdGw1QSt5M1BzbloxQ3B3S0J0eE0KRERLNWMwWitBVHVaQW53TEdTN1A0LzFXWDBSOGwxQnFuc2tSdHhYT1hNVEVjbWNZZUM2VVdaQUwrQ1FNaXB0Wgp2SDZ6YlkzUkdPRm9NanpVRmZmYnpoaWNnRGxqaTJ4d3JESWdFY2lFSjJMb1lKdW9CY1NhbmFmTEJ3MmQ5MTZuCnlMZ0pNdGhWYjZTRld1TzlxR0lkaG1hdUR2YnZwMXZsTExoWnJxMUZvNmZwUnV6QnR3PT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo="
---
# Source: airflow/templates/pgbouncer/pgbouncer-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-pgbouncer
  namespace: egov
  labels:
    app: airflow
    component: pgbouncer
data:
  pgbouncer.ini: "CltkYXRhYmFzZXNdCiogPSBob3N0PXBvc3RncmVzLmV4YW1wbGUub3JnIHBvcnQ9NTQzMgoKW3BnYm91bmNlcl0KcG9vbF9tb2RlID0gdHJhbnNhY3Rpb24KbWF4X2NsaWVudF9jb25uID0gMTAwMApkZWZhdWx0X3Bvb2xfc2l6ZSA9ICAyMAppZ25vcmVfc3RhcnR1cF9wYXJhbWV0ZXJzID0gZXh0cmFfZmxvYXRfZGlnaXRzCgpsaXN0ZW5fcG9ydCA9IDY0MzIKbGlzdGVuX2FkZHIgPSAqCgphdXRoX3R5cGUgPSBtZDUKYXV0aF9maWxlID0gL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dAoKbG9nX2Rpc2Nvbm5lY3Rpb25zID0gMApsb2dfY29ubmVjdGlvbnMgPSAwCgojIGxvY2tzIHdpbGwgbmV2ZXIgYmUgcmVsZWFzZWQgd2hlbiBgcG9vbF9tb2RlPXRyYW5zYWN0aW9uYCAoYWlyZmxvdyBpbml0ZGIvdXBncmFkZWRiIHNjcmlwdHMgY3JlYXRlIGxvY2tzKQpzZXJ2ZXJfcmVzZXRfcXVlcnkgPSBTRUxFQ1QgcGdfYWR2aXNvcnlfdW5sb2NrX2FsbCgpCnNlcnZlcl9yZXNldF9xdWVyeV9hbHdheXMgPSAxCgojIyBDTElFTlQgVExTIFNFVFRJTkdTICMjCmNsaWVudF90bHNfc3NsbW9kZSA9IHByZWZlcgpjbGllbnRfdGxzX2NpcGhlcnMgPSBub3JtYWwKY2xpZW50X3Rsc19rZXlfZmlsZSA9IC9ob21lL3BnYm91bmNlci9jZXJ0cy9jbGllbnQua2V5CmNsaWVudF90bHNfY2VydF9maWxlID0gL2hvbWUvcGdib3VuY2VyL2NlcnRzL2NsaWVudC5jcnQKCiMjIFNFUlZFUiBUTFMgU0VUVElOR1MgIyMKc2VydmVyX3Rsc19zc2xtb2RlID0gcHJlZmVyCnNlcnZlcl90bHNfY2lwaGVycyA9IG5vcm1hbA=="
  gen_auth_file.sh: "CiMhL2Jpbi9zaCAtZQoKIyBERVNDUklQVElPTjoKIyAtIHVwZGF0ZXMgdGhlIHBnYm91bmNlciBgYXV0aF9maWxlYCBmcm9tIGVudmlyb25tZW50IHZhcmlhYmxlcwojIC0gY2FsbGVkIGluIG1haW4gcGdib3VuY2VyIGNvbnRhaW5lciBzdGFydC1jb21tYW5kIHNvIHRoYXQgYGF1dGhfZmlsZWAgaXMgdXBkYXRlZCBlYWNoIHJlc3RhcnQsCiMgICBmb3IgZXhhbXBsZSwgd2hlbiB0aGUgbGl2ZW5lc3NQcm9iZSBmYWlscyBkdWUgdG8gYSBEQVRBQkFTRV9QQVNTV09SRCBzZWNyZXQgdXBkYXRlCgojIHZhcmlhYmxlcyB0byBpbmNyZWFzZSBjbGFyaXR5IG9mIHBhdHRlcm4gbWF0Y2hpbmcKT05FX1FVT1RFPSciJwpUV09fUVVPVEU9JyIiJwoKIyBwZ2JvdW5jZXIgcmVxdWlyZXMgYCJgIHRvIGJlIGVzY2FwZWQgYXMgYCIiYApFU0NBUEVEX0RBVEFCQVNFX1VTRVI9IiR7REFUQUJBU0VfVVNFUi8kT05FX1FVT1RFLyRUV09fUVVPVEV9IgpFU0NBUEVEX0RBVEFCQVNFX1BBU1NXT1JEPSIke0RBVEFCQVNFX1BBU1NXT1JELyRPTkVfUVVPVEUvJFRXT19RVU9URX0iCgojIHBnYm91bmNlciByZXF1aXJlcyBhdXRoX2ZpbGUgaW4gZm9ybWF0IGAibXktdXNlcm5hbWUiICJteS1wYXNzd29yZCJgCmVjaG8gXCIkRVNDQVBFRF9EQVRBQkFTRV9VU0VSXCIgXCIkRVNDQVBFRF9EQVRBQkFTRV9QQVNTV09SRFwiID4gL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dAplY2hvICJTdWNjZXNzZnVsbHkgZ2VuZXJhdGVkIGF1dGhfZmlsZTogL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dCI="
---
# Source: airflow/templates/redis/redis-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-redis
  namespace: egov
  labels:
    app: redis
type: Opaque
data:
  redis-password: "YWlyZmxvdw=="
---
# Source: airflow/templates/sync/sync-users-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-sync-users
  namespace: egov
  labels:
    app: airflow
    component: sync-users
data:
  sync_users.py: "
############################
#### BEGIN: GLOBAL CODE ####
############################
####################
## Global Imports ##
####################
import logging
import os
import time
from string import Template
from typing import List, Dict, Optional


####################
## Global Configs ##
####################
# the path which Secret/ConfigMap are mounted to
CONF__TEMPLATES_PATH = "/mnt/templates"

# how frequently to check for Secret/ConfigMap updates
CONF__TEMPLATES_SYNC_INTERVAL = 10

# how frequently to re-sync objects (Connections, Pools, Users, Variables)
CONF__OBJECTS_SYNC_INTERVAL = 60


######################
## Global Functions ##
######################
def string_substitution(raw_string: Optional[str], substitution_map: Dict[str, str]) -> str:
    """
    Apply bash-like substitutions to a raw string.

    Example:
    - string_substitution("Hello!", None) -> "Hello!"
    - string_substitution("Hello ${NAME}!", {"NAME": "Airflow"}) -> "Hello Airflow!"
    """
    if raw_string and len(substitution_map) > 0:
        tpl = Template(raw_string)
        return tpl.safe_substitute(substitution_map)
    else:
        return raw_string


def template_mtime(template_name: str) -> float:
    """
    Return the modification-time of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    return os.stat(file_path).st_mtime


def template_value(template_name: str) -> str:
    """
    Return the contents of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    with open(file_path, "r") as f:
        return f.read()


def refresh_template_cache(template_names: List[str],
                           template_mtime_cache: Dict[str, float],
                           template_value_cache: Dict[str, str]) -> List[str]:
    """
    Refresh the provided dictionary caches of template values & mtimes.

    :param template_names: the names of all templates to refresh
    :param template_mtime_cache: the dictionary cache of template file modification-times
    :param template_value_cache: the dictionary cache of template values
    :return: the names of templates which changed
    """
    changed_templates = []
    for template_name in template_names:
        old_mtime = template_mtime_cache.get(template_name, None)
        new_mtime = template_mtime(template_name)
        # first, check if the files were modified
        if old_mtime != new_mtime:
            old_value = template_value_cache.get(template_name, None)
            new_value = template_value(template_name)
            # second, check if the value actually changed
            if old_value != new_value:
                template_value_cache[template_name] = new_value
                changed_templates += [template_name]
            template_mtime_cache[template_name] = new_mtime
    return changed_templates


def main(sync_forever: bool):
    # initial sync of template cache
    refresh_template_cache(
        template_names=VAR__TEMPLATE_NAMES,
        template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
        template_value_cache=VAR__TEMPLATE_VALUE_CACHE
    )

    # initial sync of objects into Airflow DB
    sync_with_airflow()

    if sync_forever:
        # define variables used to track how long since last refresh/sync
        templates_sync_epoch = time.time()
        objects_sync_epoch = time.time()

        # main loop
        while True:
            # monitor for template secret/configmap updates
            if (time.time() - templates_sync_epoch) > CONF__TEMPLATES_SYNC_INTERVAL:
                logging.debug(f"template sync interval reached, re-syncing all templates...")
                changed_templates = refresh_template_cache(
                    template_names=VAR__TEMPLATE_NAMES,
                    template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
                    template_value_cache=VAR__TEMPLATE_VALUE_CACHE
                )
                templates_sync_epoch = time.time()
                if changed_templates:
                    logging.info(f"template values have changed: [{','.join(changed_templates)}]")
                    sync_with_airflow()
                    objects_sync_epoch = time.time()

            # monitor for external changes to objects (like from UI)
            if (time.time() - objects_sync_epoch) > CONF__OBJECTS_SYNC_INTERVAL:
                logging.debug(f"sync interval reached, re-syncing all objects...")
                sync_with_airflow()
                objects_sync_epoch = time.time()

            # ensure we dont loop too fast
            time.sleep(0.5)
##########################
#### END: GLOBAL CODE ####
##########################


#############
## Imports ##
#############
import sys
from flask_appbuilder.security.sqla.models import User, Role
from werkzeug.security import check_password_hash, generate_password_hash
import airflow.www.app as www_app
flask_app = www_app.create_app()
flask_appbuilder = flask_app.appbuilder


#############
## Classes ##
#############
class UserWrapper(object):
    def __init__(
            self,
            username: str,
            first_name: Optional[str] = None,
            last_name: Optional[str] = None,
            email: Optional[str] = None,
            roles: Optional[List[str]] = None,
            password: Optional[str] = None
    ):
        self.username = username
        self._first_name = first_name
        self._last_name = last_name
        self._email = email
        self.roles = roles
        self._password = password

    @property
    def first_name(self) -> str:
        return string_substitution(self._first_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def last_name(self) -> str:
        return string_substitution(self._last_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def email(self) -> str:
        return string_substitution(self._email, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def password(self) -> str:
        return string_substitution(self._password, VAR__TEMPLATE_VALUE_CACHE)

    def as_dict(self) -> Dict[str, str]:
        return {
            "username": self.username,
            "first_name": self.first_name,
            "last_name": self.last_name,
            "email": self.email,
            "roles": [find_role(role_name=role_name) for role_name in self.roles],
            "password": self.password
        }


###############
## Variables ##
###############
VAR__TEMPLATE_NAMES = [
]
VAR__TEMPLATE_MTIME_CACHE = {}
VAR__TEMPLATE_VALUE_CACHE = {}
VAR__USER_WRAPPERS = {
  "admin": UserWrapper(
    username="admin",
    first_name="admin",
    last_name="admin",
    email="admin@example.com",
    roles=[        "Admin",
    ],
    password="admin",
  ),
}


###############
## Functions ##
###############
def find_role(role_name: str) -> Role:
    """
    Get the FAB Role model associated with a `role_name`.
    """
    found_role = flask_appbuilder.sm.find_role(role_name)
    if found_role:
        return found_role
    else:
        valid_roles = flask_appbuilder.sm.get_all_roles()
        logging.error(f"Failed to find role=`{role_name}`, valid roles are: {valid_roles}")
        sys.exit(1)


def compare_role_lists(role_list_1: List[Role], role_list_2: List[Role]) -> bool:
    """
    Check if two lists of FAB Roles contain the same roles (ignores duplicates and order).
    """
    name_set_1 = set(role.name for role in role_list_1)
    name_set_2 = set(role.name for role in role_list_2)
    return name_set_1 == name_set_2



def compare_users(user_dict: Dict, user_model: User) -> bool:
    """
    Check if user info (stored in dict) is identical to a FAB User model.
    """
    return (
            user_dict["username"] == user_model.username
            and user_dict["first_name"] == user_model.first_name
            and user_dict["last_name"] == user_model.last_name
            and user_dict["email"] == user_model.email
            and compare_role_lists(user_dict["roles"], user_model.roles)
            and check_password_hash(pwhash=user_model.password, password=user_dict["password"])
    )


def sync_user(user_wrapper: UserWrapper) -> None:
    """
    Sync the User defined by a provided UserWrapper into the FAB DB.
    """
    username = user_wrapper.username
    u_new = user_wrapper.as_dict()
    u_old = flask_appbuilder.sm.find_user(username=username)

    if not u_old:
        logging.info(f"User=`{username}` is missing, adding...")
        created_user = flask_appbuilder.sm.add_user(
            username=u_new["username"],
            first_name=u_new["first_name"],
            last_name=u_new["last_name"],
            email=u_new["email"],
            # in old versions of flask_appbuilder `add_user(role=` can only add exactly one role
            # (unchecked 0 index is safe because we require at least one role using helm values validation)
            role=u_new["roles"][0],
            password=u_new["password"]
        )
        if created_user:
            # add the full list of roles (we only added the first one above)
            created_user.roles = u_new["roles"]
            logging.info(f"User=`{username}` was successfully added.")
        else:
            logging.error(f"Failed to add User=`{username}`")
            sys.exit(1)
    else:
        if compare_users(u_new, u_old):
            pass
        else:
            logging.info(f"User=`{username}` exists but has changed, updating...")
            u_old.first_name = u_new["first_name"]
            u_old.last_name = u_new["last_name"]
            u_old.email = u_new["email"]
            u_old.roles = u_new["roles"]
            u_old.password = generate_password_hash(u_new["password"])
            # strange check for False is because update_user() returns None for success
            # but in future might return the User model
            if not (flask_appbuilder.sm.update_user(u_old) is False):
                logging.info(f"User=`{username}` was successfully updated.")
            else:
                logging.error(f"Failed to update User=`{username}`")
                sys.exit(1)


def sync_all_users(user_wrappers: Dict[str, UserWrapper]) -> None:
    """
    Sync all users in provided `user_wrappers`.
    """
    logging.info("BEGIN: airflow users sync")
    for user_wrapper in user_wrappers.values():
        sync_user(user_wrapper)
    logging.info("END: airflow users sync")

    # ensures than any SQLAlchemy sessions are closed (so we don't hold a connection to the database)
    flask_app.do_teardown_appcontext()


def sync_with_airflow() -> None:
    """
    Preform a sync of all objects with airflow (note, `sync_with_airflow()` is called in `main()` template).
    """
    sync_all_users(user_wrappers=VAR__USER_WRAPPERS)


##############
## Run Main ##
##############
main(sync_forever=True)"
---
# Source: airflow/templates/redis/redis-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-redis
  namespace: egov
  labels:
    app: airflow-redis
data:
  redis.conf: |-
    # User-supplied configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  master.conf: |-
    dir /data
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
  replica.conf: |-
    dir /data
    slave-read-only yes
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
---
# Source: airflow/templates/redis/redis-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-redis-health
  namespace: egov
  labels:
    app: airflow-redis
data:
  ping_readiness_local.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_PASSWORD --no-auth-warning \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_PASSWORD --no-auth-warning \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_MASTER_PASSWORD --no-auth-warning \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_MASTER_PASSWORD --no-auth-warning \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: airflow/templates/rbac/airflow-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: airflow
  namespace: egov
  labels:
    app: airflow
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - "create"
  - "get"
  - "delete"
  - "list"
  - "patch"
  - "watch"
- apiGroups:
  - ""
  resources:
  - "pods/log"
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - "pods/exec"
  verbs:
  - "create"
  - "get"
---
# Source: airflow/templates/rbac/airflow-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: airflow
  namespace: egov
  labels:
    app: airflow
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: airflow
subjects:
- kind: ServiceAccount
  name: airflow
  namespace: egov
---
# Source: airflow/templates/flower/flower-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-flower
  namespace: egov
  labels:
    app: airflow
    component: flower
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: flower
  ports:
    - name: flower
      protocol: TCP
      port: 5555
      targetPort: 5555
---
# Source: airflow/templates/pgbouncer/pgbouncer-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-pgbouncer
  namespace: egov
  labels:
    app: airflow
    component: pgbouncer
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: pgbouncer
  ports:
    - name: pgbouncer
      protocol: TCP
      port: 6432
---
# Source: airflow/templates/redis/redis-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-redis-headless
  namespace: egov
  labels:
    app: airflow-redis
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: airflow-redis
---
# Source: airflow/templates/redis/redis-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-redis-master
  namespace: egov
  labels:
    app: airflow-redis
spec:
  type: ClusterIP
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: airflow-redis
---
# Source: airflow/templates/webserver/webserver-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-web
  namespace: egov
  labels:
    app: airflow
    component: web
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: web
  sessionAffinity: None
  ports:
    - name: web
      protocol: TCP
      port: 8080
      targetPort: 8080
---
# Source: airflow/templates/worker/worker-service.yaml
apiVersion: v1
## this Service gives stable DNS entries for workers, used by webserver for logs
kind: Service
metadata:
  name: airflow-worker
  namespace: egov
  labels:
    app: airflow
    component: worker
spec:
  ports:
    - name: worker
      protocol: TCP
      port: 8793
  clusterIP: None
  selector:
    app: airflow
    component: worker
---
# Source: airflow/templates/db-migrations/db-migrations-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-db-migrations
  namespace: egov
  labels:
    app: airflow
    component: db-migrations
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: db-migrations
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: db-migrations
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: db-migrations          
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/db_migrations.py"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
        - name: scripts
          secret:
            secretName: airflow-db-migrations
---
# Source: airflow/templates/flower/flower-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-flower
  namespace: egov
  labels:
    app: airflow
    component: flower
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple flower pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: flower
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: flower
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-flower          
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          ports:
            - name: flower
              containerPort: 5555
              protocol: TCP
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow celery flower"
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            exec:
              command:
                - "bash"
                - "-c"
                - "exec curl 'http://localhost:5555/airflow/flower'"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            exec:
              command:
                - "bash"
                - "-c"
                - "exec curl 'http://localhost:5555/airflow/flower'"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
---
# Source: airflow/templates/pgbouncer/pgbouncer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-pgbouncer
  namespace: egov
  labels:
    app: airflow
    component: pgbouncer
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      ## multiple pgbouncer pods can safely run concurrently
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: pgbouncer
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: pgbouncer
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      terminationGracePeriodSeconds: 120
      serviceAccountName: airflow
      containers:
        - name: pgbouncer
          image: ghcr.io/airflow-helm/pgbouncer:1.17.0-patch.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 1001
            runAsGroup: 1001
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          ports:
            - name: pgbouncer
              containerPort: 6432
              protocol: TCP
          command:
            - "/usr/bin/dumb-init"
            ## rewrite SIGTERM as SIGINT, so pgbouncer does a safe shutdown
            - "--rewrite=15:2"
            - "--"
          args:
            - "/bin/sh"
            - "-c"
            ## we generate users.txt on startup, because DATABASE_PASSWORD is defined from a Secret,
            ## and we want to pickup the new values on container restart (possibly due to livenessProbe failure)
            - |-
              /home/pgbouncer/config/gen_auth_file.sh && \
              exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 60
            failureThreshold: 3
            exec:
              command:
                - "/bin/sh"
                - "-c"
                ## this check is intended to fail when the DATABASE_PASSWORD secret is updated,
                ## which would cause `gen_auth_file.sh` to run again on container start
                - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;" | grep -q "1"
          startupProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 15
            failureThreshold: 30
            tcpSocket:
              port: 6432
          volumeMounts:
            - name: pgbouncer-config
              mountPath: /home/pgbouncer/config
              readOnly: true
            - name: pgbouncer-certs
              mountPath: /home/pgbouncer/certs
              readOnly: true
      volumes:
        - name: pgbouncer-config
          secret:
            secretName: airflow-pgbouncer
            items:
              - key: gen_auth_file.sh
                path: gen_auth_file.sh
                mode: 0755
              - key: pgbouncer.ini
                path: pgbouncer.ini
        - name: pgbouncer-certs
          projected:
            sources:
              ## CLIENT TLS FILES (CHART GENERATED)
              - secret:
                  name: airflow-pgbouncer-certs
                  items:
                    - key: client.key
                      path: client.key
                    - key: client.crt
                      path: client.crt
---
# Source: airflow/templates/scheduler/scheduler-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  namespace: egov
  labels:
    app: airflow
    component: scheduler
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple schedulers can run concurrently (Airflow 2.0)
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: scheduler
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: scheduler
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-scheduler          
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow scheduler -n -1"
          livenessProbe:
            initialDelaySeconds: 60
            periodSeconds: 30
            failureThreshold: 5
            timeoutSeconds: 60
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys

                  # suppress logs triggered from importing airflow packages
                  os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                  from airflow.jobs.scheduler_job import SchedulerJob
                  from airflow.utils.db import create_session
                  from airflow.utils.net import get_hostname

                  with create_session() as session:
                      # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                      hostname = get_hostname()
                      scheduler_job = session \
                          .query(SchedulerJob) \
                          .filter_by(hostname=hostname) \
                          .order_by(SchedulerJob.latest_heartbeat.desc()) \
                          .limit(1) \
                          .first()
                      if (scheduler_job is not None) and scheduler_job.is_alive():
                          pass
                      else:
                          sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: log-cleanup  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: LOG_PATH
              value: "/opt/airflow/logs"
            - name: RETENTION_MINUTES
              value: "21600"
            - name: INTERVAL_SECONDS
              value: "900"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - |
              set -euo pipefail
        
              # break the infinite loop when we receive SIGINT or SIGTERM
              trap "exit 0" SIGINT SIGTERM
        
              while true; do
                START_EPOCH=$(date --utc +%s)
                echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."
        
                # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
                # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
                DELETED_COUNT=$(
                  find "$LOG_PATH" \
                    -type f \
                    -name "*.log" \
                    -mmin +"$RETENTION_MINUTES" \
                    -writable \
                    -delete \
                    -printf "." \
                  | wc -c
                )
        
                END_EPOCH=$(date --utc +%s)
                LOOP_DURATION=$((END_EPOCH - START_EPOCH))
                echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"
        
                SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
                if (( SECONDS_TO_SLEEP > 0 )); then
                  echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                  sleep $SECONDS_TO_SLEEP
                fi
              done
          volumeMounts:
            - name: logs-data
              mountPath: /opt/airflow/logs
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
---
# Source: airflow/templates/sync/sync-users-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-sync-users
  namespace: egov
  labels:
    app: airflow
    component: sync-users
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: sync-users
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: sync-users
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: sync-airflow-users          
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/sync_users.py"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
        - name: scripts
          secret:
            secretName: airflow-sync-users
---
# Source: airflow/templates/triggerer/triggerer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-triggerer
  labels:
    app: airflow
    component: triggerer
    chart: airflow-8.5.3
    release: RELEASE-NAME
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple triggerer pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: triggerer
      release: RELEASE-NAME
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: c3ce4924427f1d6f9d2c6a73a6b6c8cb937dcaeb0d3a03df8424b9c92034ee8e
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: triggerer
        release: RELEASE-NAME
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      serviceAccountName: airflow
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-triggerer          
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow triggerer"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 60
            failureThreshold: 5
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys

                  # suppress logs triggered from importing airflow packages
                  os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                  from airflow.jobs.triggerer_job import TriggererJob
                  from airflow.utils.db import create_session
                  from airflow.utils.net import get_hostname

                  with create_session() as session:
                      # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                      hostname = get_hostname()
                      triggerer_job = session \
                          .query(TriggererJob) \
                          .filter_by(hostname=hostname) \
                          .order_by(TriggererJob.latest_heartbeat.desc()) \
                          .limit(1) \
                          .first()
                      if (triggerer_job is not None) and triggerer_job.is_alive():
                          pass
                      else:
                          sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
---
# Source: airflow/templates/webserver/webserver-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-web
  namespace: egov
  labels:
    app: airflow
    component: web
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple web pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: web
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: web
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      serviceAccountName: airflow
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-web          
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          ports:
            - name: web
              containerPort: 8080
              protocol: TCP
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow webserver"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /airflow/health
              port: web
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /airflow/health
              port: web
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: webserver-config
              mountPath: /opt/airflow/webserver_config.py
              subPath: webserver_config.py
              readOnly: true        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
        - name: webserver-config
          secret:
            secretName: airflow-webserver-config
            defaultMode: 0644
---
# Source: airflow/templates/redis/redis-master.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: airflow-redis-master
  namespace: egov
  labels:
    app: airflow-redis
spec:
  selector:
    matchLabels:
      app: airflow-redis
  serviceName: airflow-redis-headless
  template:
    metadata:
      labels:
        app: airflow-redis
    spec:      
      securityContext:
        fsGroup: 1001
      serviceAccountName: "default"
      containers:
      - name: airflow-redis
        image: "docker.io/bitnami/redis:5.0.7-debian-10-r32"
        imagePullPolicy: "IfNotPresent"
        securityContext:
          runAsUser: 1001
        command:
        - /bin/bash
        - -c
        - |
          if [[ -n $REDIS_PASSWORD_FILE ]]; then
            password_aux=`cat ${REDIS_PASSWORD_FILE}`
            export REDIS_PASSWORD=$password_aux
          fi
          if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
          fi
          if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
          fi
          ARGS=("--port" "${REDIS_PORT}")
          ARGS+=("--requirepass" "${REDIS_PASSWORD}")
          ARGS+=("--masterauth" "${REDIS_PASSWORD}")
          ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
          ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
          /run.sh ${ARGS[@]}
        env:
        - name: REDIS_REPLICATION_MODE
          value: master
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: airflow-redis
              key: redis-password
        - name: REDIS_PORT
          value: "6379"
        ports:
        - name: redis
          containerPort: 6379
        livenessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_liveness_local.sh 5
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_readiness_local.sh 5
        resources:
          {}
        volumeMounts:
        - name: health
          mountPath: /health
        - name: redis-data
          mountPath: /data
          subPath: 
        - name: config
          mountPath: /opt/bitnami/redis/mounted-etc
        - name: redis-tmp-conf
          mountPath: /opt/bitnami/redis/etc/
      volumes:
      - name: health
        configMap:
          name: airflow-redis-health
          defaultMode: 0755
      - name: config
        configMap:
          name: airflow-redis
      - name: "redis-data"
        emptyDir: {}
      - name: redis-tmp-conf
        emptyDir: {}
  updateStrategy:
    type: RollingUpdate
---
# Source: airflow/templates/worker/worker-statefulset.yaml
apiVersion: apps/v1
## StatefulSet gives workers consistent DNS names, allowing webserver access to log files
kind: StatefulSet
metadata:
  name: airflow-worker
  namespace: egov
  labels:
    app: airflow
    component: worker
spec:
  serviceName: "airflow-worker"
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  ## we do not need to guarantee the order in which workers are scaled
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: airflow
      component: worker
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: worker
    spec:
      restartPolicy: Always
      terminationGracePeriodSeconds: 60
      serviceAccountName: airflow
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-worker          
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
            # have dumb-init only send signals to direct child process (needed for celery workers to warm shutdown)
            - name: DUMB_INIT_SETSID
              value: "0"
          ports:
            - name: wlog
              containerPort: 8793
              protocol: TCP
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow celery worker"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/egovernments/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"    
            - name: scheduler_health_check_threshold
              value: "240"
            - name: orphaned_tasks_check_interval 
              value: "300"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: log-cleanup  
          image: apache/airflow:2.2.5-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: LOG_PATH
              value: "/opt/airflow/logs"
            - name: RETENTION_MINUTES
              value: "21600"
            - name: INTERVAL_SECONDS
              value: "900"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - |
              set -euo pipefail
        
              # break the infinite loop when we receive SIGINT or SIGTERM
              trap "exit 0" SIGINT SIGTERM
        
              while true; do
                START_EPOCH=$(date --utc +%s)
                echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."
        
                # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
                # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
                DELETED_COUNT=$(
                  find "$LOG_PATH" \
                    -type f \
                    -name "*.log" \
                    -mmin +"$RETENTION_MINUTES" \
                    -writable \
                    -delete \
                    -printf "." \
                  | wc -c
                )
        
                END_EPOCH=$(date --utc +%s)
                LOOP_DURATION=$((END_EPOCH - START_EPOCH))
                echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"
        
                SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
                if (( SECONDS_TO_SLEEP > 0 )); then
                  echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                  sleep $SECONDS_TO_SLEEP
                fi
              done
          volumeMounts:
            - name: logs-data
              mountPath: /opt/airflow/logs
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
---
# Source: airflow/templates/flower/flower-ingress-v1beta1.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: airflow-flower
  namespace: egov
  labels:
    app: airflow
    component: flower
spec:
  tls:
    - hosts:
        - qa.digit.org
      secretName: qa.digit.org-tls-certs  
  rules:
    - host: qa.digit.org
      http:
        paths:
          - path: /airflow/flower
            backend:
              serviceName: airflow-flower
              servicePort: flower
---
# Source: airflow/templates/webserver/webserver-ingress-v1beta1.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: airflow-web
  namespace: egov
  labels:
    app: airflow
    component: web
spec:
  tls:
    - hosts:
        - qa.digit.org
      secretName: qa.digit.org-tls-certs  
  rules:
    - host: qa.digit.org
      http:
        paths:
          - path: /airflow
            backend:
              serviceName: airflow-web
              servicePort: web

