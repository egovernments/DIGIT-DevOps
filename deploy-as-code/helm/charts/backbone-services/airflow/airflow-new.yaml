---
# Source: airflow/templates/rbac/airflow-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: airflow
  namespace: egov
  labels:
    app: airflow
---
# Source: airflow/templates/config/secret-config-envs.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-config-envs
  namespace: egov
  labels:
    app: airflow
## we must use `data` rather than `stringData` (see: https://github.com/helm/helm/issues/10010)
data:
  ## ================
  ## Linux Configs
  ## ================
  TZ: "RXRjL1VUQw=="

  ## ================
  ## Database Configs
  ## ================
  ## database host/port
  DATABASE_HOST: "YWlyZmxvdy1wZ2JvdW5jZXIuZWdvdi5zdmMuY2x1c3Rlci5sb2NhbA=="
  DATABASE_PORT: "NjQzMg=="

  ## database configs

  ## bash command which echos the URL encoded value of $DATABASE_USER
  DATABASE_USER_CMD: "ZWNobyAiJHtEQVRBQkFTRV9VU0VSfSIgfCBweXRob24zIC1jICJpbXBvcnQgdXJsbGliLnBhcnNlOyBlbmNvZGVkX3VzZXIgPSB1cmxsaWIucGFyc2UucXVvdGUoaW5wdXQoKSk7IHByaW50KGVuY29kZWRfdXNlciki"

  ## bash command which echos the URL encoded value of $DATABASE_PASSWORD
  DATABASE_PASSWORD_CMD: "ZWNobyAiJHtEQVRBQkFTRV9QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChlbmNvZGVkX3Bhc3MpIg=="

  ## bash command which echos the DB connection string in SQLAlchemy format
  DATABASE_SQLALCHEMY_CMD: "ZWNobyAtbiAicG9zdGdyZXNxbCtwc3ljb3BnMjovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0ke0RBVEFCQVNFX1BST1BFUlRJRVN9Ig=="

  ## bash command which echos the DB connection string in Celery result_backend format
  DATABASE_CELERY_CMD: "ZWNobyAtbiAiZGIrcG9zdGdyZXNxbDovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0ke0RBVEFCQVNFX1BST1BFUlRJRVN9Ig=="

  ## bash command which echos the DB connection string in `psql` cli format
  DATABASE_PSQL_CMD: "ZWNobyAtbiAicG9zdGdyZXNxbDovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0ke0RBVEFCQVNFX1BST1BFUlRJRVN9Ig=="

  ## ================
  ## Redis Configs
  ## ================
  ## connection string components

  ## credentials (from plain-text helm values)
  REDIS_PASSWORD: ""

  ## bash command which echos the URL encoded value of $REDIS_PASSWORD
  ## NOTE: if $REDIS_PASSWORD is non-empty, prints `:${REDIS_PASSWORD}@`, else ``
  REDIS_PASSWORD_CMD: "ZWNobyAiJHtSRURJU19QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChmXCI6e2VuY29kZWRfcGFzc31AXCIpIGlmIGxlbihlbmNvZGVkX3Bhc3MpID4gMCBlbHNlIE5vbmUi"

  ## bash command which echos the Redis connection string
  REDIS_CONNECTION_CMD: "ZWNobyAtbiAicmVkaXM6Ly8kKGV2YWwgJFJFRElTX1BBU1NXT1JEX0NNRCkke1JFRElTX0hPU1R9OiR7UkVESVNfUE9SVH0vJHtSRURJU19EQk5VTX0ke1JFRElTX1BST1BFUlRJRVN9Ig=="

  ## ================
  ## Airflow Configs (General)
  ## ================
  AIRFLOW__CORE__DAGS_FOLDER: "L29wdC9haXJmbG93L2RhZ3MvcmVwby9lZ292LW5hdGlvbmFsLWRhc2hib2FyZC1hY2NlbGVyYXRvci9kYWdz"
  AIRFLOW__CORE__EXECUTOR: "Q2VsZXJ5RXhlY3V0b3I="
  AIRFLOW__CORE__FERNET_KEY: "N1Q1MTJVWFNTbUJPa3BXaW1GSElWYjhqSzZsZm1TQXZ4NG1PNkFyZWhuYz0="
  AIRFLOW__CORE__SQL_ALCHEMY_CONN_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX1NRTEFMQ0hFTVlfQ01EIic="
  AIRFLOW__WEBSERVER__SECRET_KEY: "VEhJUyBJUyBVTlNBRkUh"
  AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "ODA4MA=="
  AIRFLOW__CELERY__FLOWER_PORT: "NTU1NQ=="
  ## refresh the dags folder at the same frequency as git-sync
  AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "NjA="

  ## ================
  ## Airflow Configs (Logging)
  ## ================
  AIRFLOW__LOGGING__BASE_LOG_FOLDER: "L29wdC9haXJmbG93L2xvZ3M="
  AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION: "L29wdC9haXJmbG93L2xvZ3MvZGFnX3Byb2Nlc3Nvcl9tYW5hZ2VyL2RhZ19wcm9jZXNzb3JfbWFuYWdlci5sb2c="
  AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY: "L29wdC9haXJmbG93L2xvZ3Mvc2NoZWR1bGVy"

  ## ================
  ## Airflow Configs (Celery)
  ## ================
  AIRFLOW__CELERY__WORKER_LOG_SERVER_PORT: "ODc5Mw=="
  AIRFLOW__CELERY__BROKER_URL_CMD: "YmFzaCAtYyAnZXZhbCAiJFJFRElTX0NPTk5FQ1RJT05fQ01EIic="
  AIRFLOW__CELERY__RESULT_BACKEND_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX0NFTEVSWV9DTUQiJw=="

  ## ================
  ## Airflow Configs (Kubernetes)
  ## ================

  ## ================
  ## User Configs
  ## ================
---
# Source: airflow/templates/config/secret-webserver-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-webserver-config
  namespace: egov
  labels:
    app: airflow
data:
  webserver_config.py: "ZnJvbSBhaXJmbG93IGltcG9ydCBjb25maWd1cmF0aW9uIGFzIGNvbmYKZnJvbSBmbGFza19hcHBidWlsZGVyLnNlY3VyaXR5Lm1hbmFnZXIgaW1wb3J0IEFVVEhfREIKCiMgdGhlIFNRTEFsY2hlbXkgY29ubmVjdGlvbiBzdHJpbmcKU1FMQUxDSEVNWV9EQVRBQkFTRV9VUkkgPSBjb25mLmdldCgnY29yZScsICdTUUxfQUxDSEVNWV9DT05OJykKCiMgdXNlIGVtYmVkZGVkIERCIGZvciBhdXRoCkFVVEhfVFlQRSA9IEFVVEhfREIK"
---
# Source: airflow/templates/db-migrations/db-migrations-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-db-migrations
  namespace: egov
  labels:
    app: airflow
    component: db-migrations
data:
  db_migrations.py: "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"
---
# Source: airflow/templates/pgbouncer/pgbouncer-secret-certs.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-pgbouncer-certs
  namespace: egov
  labels:
    app: airflow
    component: pgbouncer
data:
  client.key: "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"
  client.crt: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQWRxZ0F3SUJBZ0lSQUxIRlhUUEpqRnM2bUVSa1Nac0xaMXN3RFFZSktvWklodmNOQVFFTEJRQXcKRkRFU01CQUdBMVVFQXhNSmJHOWpZV3hvYjNOME1CNFhEVEl5TURVeU9URTRORGt5TTFvWERUSXpNRFV5T1RFNApORGt5TTFvd0ZERVNNQkFHQTFVRUF4TUpiRzlqWVd4b2IzTjBNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DCkFROEFNSUlCQ2dLQ0FRRUF4WCtCRGZoL05jd0RreDhNTHpsd3EwKythRStvMlVnVXFsVCtJVDZYbC9GNjh4cnoKSlYxeGFLNEc2YjBEbUJRZFBWTHZQaFVaY1BBUUN2R2xMQ3JNUFZaN1h2Rk5MdG5aZFQ5RzBQMGlheWF3VWV1NQpKTHhpWVdralQ5UFNROWNqYmxOOUNYbWpSd0kwVU4wYkYzSjBEQ2NtdXJDNUFuYmJDMkdBS1hXM3l4a3FEc1d4CkxTc2UyTW1jTVFSWGdJdmV3dEZQRi92QnlrSFpwTmZJQUNhdjNwUjNZV3hvRkNtT1hPTlJHS0F3cEY2cGxnRFgKRUdqWmZCU041eU4xTnhpNUNNZ1ZXVVh4MzJYaThKUkFOZ2dJaGNYWS9nUGE2QUVuWUUvZnFRdmFtVXRxSVcvdAp1M0twbm5GMWFJMjB5cnlKQ1k4KzZGRXZ5Z01CQ21LYm9DOTBVd0lEQVFBQm96OHdQVEFPQmdOVkhROEJBZjhFCkJBTUNCYUF3SFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUMKTUFBd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFIMHNmNUIvRjkrWnNMczliVytKeXBWRnhKbVl3dmJUcnY1ZwpSd1VwVys3eGNOMEtTdndiUEFENDkyUXlKWXh1QkJCcXo0Y1JLeXpzdldFaTZxNmNHV0dJajl2VmZPekNjd3Y3ClZmYkhtTldvMllCN0tqWXVyTnJFMStBWU1wUng4THhZZUQrR3lTUTNCcnVLUmVEdDU3bkgvUG9NRFZRd0RmalUKUGlEZUJqSzZwR1o2U3dWMVBjcmFySzI3ZlNKc3ZrZHZVWE84STR5TG8rTGRBdTZEV2dvNENIcCtaYkZ3WS9NVQowQ1AxSnBsamtORUJKN2RDNjAzUC9sVWMzeGI5NEdzbFIvVmtQZWlpUTc0RytnU29lVHozSmpnOFp0VUhOZmp5CitCMDZuY3RjUjJhMmdCcWZpeDdsUGZCa3o2dG9rZkh1SEZuZHN0UlltRThVRzFYZVplbz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo="
---
# Source: airflow/templates/pgbouncer/pgbouncer-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-pgbouncer
  namespace: egov
  labels:
    app: airflow
    component: pgbouncer
data:
  pgbouncer.ini: "CltkYXRhYmFzZXNdCiogPSBob3N0PXBvc3RncmVzLmV4YW1wbGUub3JnIHBvcnQ9NTQzMgoKW3BnYm91bmNlcl0KcG9vbF9tb2RlID0gdHJhbnNhY3Rpb24KbWF4X2NsaWVudF9jb25uID0gMTAwMApkZWZhdWx0X3Bvb2xfc2l6ZSA9ICAyMAppZ25vcmVfc3RhcnR1cF9wYXJhbWV0ZXJzID0gZXh0cmFfZmxvYXRfZGlnaXRzCgpsaXN0ZW5fcG9ydCA9IDY0MzIKbGlzdGVuX2FkZHIgPSAqCgphdXRoX3R5cGUgPSBtZDUKYXV0aF9maWxlID0gL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dAoKbG9nX2Rpc2Nvbm5lY3Rpb25zID0gMApsb2dfY29ubmVjdGlvbnMgPSAwCgojIGxvY2tzIHdpbGwgbmV2ZXIgYmUgcmVsZWFzZWQgd2hlbiBgcG9vbF9tb2RlPXRyYW5zYWN0aW9uYCAoYWlyZmxvdyBpbml0ZGIvdXBncmFkZWRiIHNjcmlwdHMgY3JlYXRlIGxvY2tzKQpzZXJ2ZXJfcmVzZXRfcXVlcnkgPSBTRUxFQ1QgcGdfYWR2aXNvcnlfdW5sb2NrX2FsbCgpCnNlcnZlcl9yZXNldF9xdWVyeV9hbHdheXMgPSAxCgojIyBDTElFTlQgVExTIFNFVFRJTkdTICMjCmNsaWVudF90bHNfc3NsbW9kZSA9IHByZWZlcgpjbGllbnRfdGxzX2NpcGhlcnMgPSBub3JtYWwKY2xpZW50X3Rsc19rZXlfZmlsZSA9IC9ob21lL3BnYm91bmNlci9jZXJ0cy9jbGllbnQua2V5CmNsaWVudF90bHNfY2VydF9maWxlID0gL2hvbWUvcGdib3VuY2VyL2NlcnRzL2NsaWVudC5jcnQKCiMjIFNFUlZFUiBUTFMgU0VUVElOR1MgIyMKc2VydmVyX3Rsc19zc2xtb2RlID0gcHJlZmVyCnNlcnZlcl90bHNfY2lwaGVycyA9IG5vcm1hbA=="
  gen_auth_file.sh: "CiMhL2Jpbi9zaCAtZQoKIyBERVNDUklQVElPTjoKIyAtIHVwZGF0ZXMgdGhlIHBnYm91bmNlciBgYXV0aF9maWxlYCBmcm9tIGVudmlyb25tZW50IHZhcmlhYmxlcwojIC0gY2FsbGVkIGluIG1haW4gcGdib3VuY2VyIGNvbnRhaW5lciBzdGFydC1jb21tYW5kIHNvIHRoYXQgYGF1dGhfZmlsZWAgaXMgdXBkYXRlZCBlYWNoIHJlc3RhcnQsCiMgICBmb3IgZXhhbXBsZSwgd2hlbiB0aGUgbGl2ZW5lc3NQcm9iZSBmYWlscyBkdWUgdG8gYSBEQVRBQkFTRV9QQVNTV09SRCBzZWNyZXQgdXBkYXRlCgojIHZhcmlhYmxlcyB0byBpbmNyZWFzZSBjbGFyaXR5IG9mIHBhdHRlcm4gbWF0Y2hpbmcKT05FX1FVT1RFPSciJwpUV09fUVVPVEU9JyIiJwoKIyBwZ2JvdW5jZXIgcmVxdWlyZXMgYCJgIHRvIGJlIGVzY2FwZWQgYXMgYCIiYApFU0NBUEVEX0RBVEFCQVNFX1VTRVI9IiR7REFUQUJBU0VfVVNFUi8kT05FX1FVT1RFLyRUV09fUVVPVEV9IgpFU0NBUEVEX0RBVEFCQVNFX1BBU1NXT1JEPSIke0RBVEFCQVNFX1BBU1NXT1JELyRPTkVfUVVPVEUvJFRXT19RVU9URX0iCgojIHBnYm91bmNlciByZXF1aXJlcyBhdXRoX2ZpbGUgaW4gZm9ybWF0IGAibXktdXNlcm5hbWUiICJteS1wYXNzd29yZCJgCmVjaG8gXCIkRVNDQVBFRF9EQVRBQkFTRV9VU0VSXCIgXCIkRVNDQVBFRF9EQVRBQkFTRV9QQVNTV09SRFwiID4gL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dAplY2hvICJTdWNjZXNzZnVsbHkgZ2VuZXJhdGVkIGF1dGhfZmlsZTogL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dCI="
---
# Source: airflow/templates/sync/sync-users-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-sync-users
  namespace: egov
  labels:
    app: airflow
    component: sync-users
data:
  sync_users.py: "
############################
#### BEGIN: GLOBAL CODE ####
############################
####################
## Global Imports ##
####################
import logging
import os
import time
from string import Template
from typing import List, Dict, Optional


####################
## Global Configs ##
####################
# the path which Secret/ConfigMap are mounted to
CONF__TEMPLATES_PATH = "/mnt/templates"

# how frequently to check for Secret/ConfigMap updates
CONF__TEMPLATES_SYNC_INTERVAL = 10

# how frequently to re-sync objects (Connections, Pools, Users, Variables)
CONF__OBJECTS_SYNC_INTERVAL = 60


######################
## Global Functions ##
######################
def string_substitution(raw_string: Optional[str], substitution_map: Dict[str, str]) -> str:
    """
    Apply bash-like substitutions to a raw string.

    Example:
    - string_substitution("Hello!", None) -> "Hello!"
    - string_substitution("Hello ${NAME}!", {"NAME": "Airflow"}) -> "Hello Airflow!"
    """
    if raw_string and len(substitution_map) > 0:
        tpl = Template(raw_string)
        return tpl.safe_substitute(substitution_map)
    else:
        return raw_string


def template_mtime(template_name: str) -> float:
    """
    Return the modification-time of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    return os.stat(file_path).st_mtime


def template_value(template_name: str) -> str:
    """
    Return the contents of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    with open(file_path, "r") as f:
        return f.read()


def refresh_template_cache(template_names: List[str],
                           template_mtime_cache: Dict[str, float],
                           template_value_cache: Dict[str, str]) -> List[str]:
    """
    Refresh the provided dictionary caches of template values & mtimes.

    :param template_names: the names of all templates to refresh
    :param template_mtime_cache: the dictionary cache of template file modification-times
    :param template_value_cache: the dictionary cache of template values
    :return: the names of templates which changed
    """
    changed_templates = []
    for template_name in template_names:
        old_mtime = template_mtime_cache.get(template_name, None)
        new_mtime = template_mtime(template_name)
        # first, check if the files were modified
        if old_mtime != new_mtime:
            old_value = template_value_cache.get(template_name, None)
            new_value = template_value(template_name)
            # second, check if the value actually changed
            if old_value != new_value:
                template_value_cache[template_name] = new_value
                changed_templates += [template_name]
            template_mtime_cache[template_name] = new_mtime
    return changed_templates


def main(sync_forever: bool):
    # initial sync of template cache
    refresh_template_cache(
        template_names=VAR__TEMPLATE_NAMES,
        template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
        template_value_cache=VAR__TEMPLATE_VALUE_CACHE
    )

    # initial sync of objects into Airflow DB
    sync_with_airflow()

    if sync_forever:
        # define variables used to track how long since last refresh/sync
        templates_sync_epoch = time.time()
        objects_sync_epoch = time.time()

        # main loop
        while True:
            # monitor for template secret/configmap updates
            if (time.time() - templates_sync_epoch) > CONF__TEMPLATES_SYNC_INTERVAL:
                logging.debug(f"template sync interval reached, re-syncing all templates...")
                changed_templates = refresh_template_cache(
                    template_names=VAR__TEMPLATE_NAMES,
                    template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
                    template_value_cache=VAR__TEMPLATE_VALUE_CACHE
                )
                templates_sync_epoch = time.time()
                if changed_templates:
                    logging.info(f"template values have changed: [{','.join(changed_templates)}]")
                    sync_with_airflow()
                    objects_sync_epoch = time.time()

            # monitor for external changes to objects (like from UI)
            if (time.time() - objects_sync_epoch) > CONF__OBJECTS_SYNC_INTERVAL:
                logging.debug(f"sync interval reached, re-syncing all objects...")
                sync_with_airflow()
                objects_sync_epoch = time.time()

            # ensure we dont loop too fast
            time.sleep(0.5)
##########################
#### END: GLOBAL CODE ####
##########################


#############
## Imports ##
#############
import sys
from flask_appbuilder.security.sqla.models import User, Role
from werkzeug.security import check_password_hash, generate_password_hash
import airflow.www.app as www_app
flask_app = www_app.create_app()
flask_appbuilder = flask_app.appbuilder


#############
## Classes ##
#############
class UserWrapper(object):
    def __init__(
            self,
            username: str,
            first_name: Optional[str] = None,
            last_name: Optional[str] = None,
            email: Optional[str] = None,
            roles: Optional[List[str]] = None,
            password: Optional[str] = None
    ):
        self.username = username
        self._first_name = first_name
        self._last_name = last_name
        self._email = email
        self.roles = roles
        self._password = password

    @property
    def first_name(self) -> str:
        return string_substitution(self._first_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def last_name(self) -> str:
        return string_substitution(self._last_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def email(self) -> str:
        return string_substitution(self._email, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def password(self) -> str:
        return string_substitution(self._password, VAR__TEMPLATE_VALUE_CACHE)

    def as_dict(self) -> Dict[str, str]:
        return {
            "username": self.username,
            "first_name": self.first_name,
            "last_name": self.last_name,
            "email": self.email,
            "roles": [find_role(role_name=role_name) for role_name in self.roles],
            "password": self.password
        }


###############
## Variables ##
###############
VAR__TEMPLATE_NAMES = [
]
VAR__TEMPLATE_MTIME_CACHE = {}
VAR__TEMPLATE_VALUE_CACHE = {}
VAR__USER_WRAPPERS = {
  "admin": UserWrapper(
    username="admin",
    first_name="admin",
    last_name="admin",
    email="admin@example.com",
    roles=[        "Admin",
    ],
    password="admin",
  ),
}


###############
## Functions ##
###############
def find_role(role_name: str) -> Role:
    """
    Get the FAB Role model associated with a `role_name`.
    """
    found_role = flask_appbuilder.sm.find_role(role_name)
    if found_role:
        return found_role
    else:
        valid_roles = flask_appbuilder.sm.get_all_roles()
        logging.error(f"Failed to find role=`{role_name}`, valid roles are: {valid_roles}")
        sys.exit(1)


def compare_role_lists(role_list_1: List[Role], role_list_2: List[Role]) -> bool:
    """
    Check if two lists of FAB Roles contain the same roles (ignores duplicates and order).
    """
    name_set_1 = set(role.name for role in role_list_1)
    name_set_2 = set(role.name for role in role_list_2)
    return name_set_1 == name_set_2



def compare_users(user_dict: Dict, user_model: User) -> bool:
    """
    Check if user info (stored in dict) is identical to a FAB User model.
    """
    return (
            user_dict["username"] == user_model.username
            and user_dict["first_name"] == user_model.first_name
            and user_dict["last_name"] == user_model.last_name
            and user_dict["email"] == user_model.email
            and compare_role_lists(user_dict["roles"], user_model.roles)
            and check_password_hash(pwhash=user_model.password, password=user_dict["password"])
    )


def sync_user(user_wrapper: UserWrapper) -> None:
    """
    Sync the User defined by a provided UserWrapper into the FAB DB.
    """
    username = user_wrapper.username
    u_new = user_wrapper.as_dict()
    u_old = flask_appbuilder.sm.find_user(username=username)

    if not u_old:
        logging.info(f"User=`{username}` is missing, adding...")
        created_user = flask_appbuilder.sm.add_user(
            username=u_new["username"],
            first_name=u_new["first_name"],
            last_name=u_new["last_name"],
            email=u_new["email"],
            # in old versions of flask_appbuilder `add_user(role=` can only add exactly one role
            # (unchecked 0 index is safe because we require at least one role using helm values validation)
            role=u_new["roles"][0],
            password=u_new["password"]
        )
        if created_user:
            # add the full list of roles (we only added the first one above)
            created_user.roles = u_new["roles"]
            logging.info(f"User=`{username}` was successfully added.")
        else:
            logging.error(f"Failed to add User=`{username}`")
            sys.exit(1)
    else:
        if compare_users(u_new, u_old):
            pass
        else:
            logging.info(f"User=`{username}` exists but has changed, updating...")
            u_old.first_name = u_new["first_name"]
            u_old.last_name = u_new["last_name"]
            u_old.email = u_new["email"]
            u_old.roles = u_new["roles"]
            u_old.password = generate_password_hash(u_new["password"])
            # strange check for False is because update_user() returns None for success
            # but in future might return the User model
            if not (flask_appbuilder.sm.update_user(u_old) is False):
                logging.info(f"User=`{username}` was successfully updated.")
            else:
                logging.error(f"Failed to update User=`{username}`")
                sys.exit(1)


def sync_all_users(user_wrappers: Dict[str, UserWrapper]) -> None:
    """
    Sync all users in provided `user_wrappers`.
    """
    logging.info("BEGIN: airflow users sync")
    for user_wrapper in user_wrappers.values():
        sync_user(user_wrapper)
    logging.info("END: airflow users sync")

    # ensures than any SQLAlchemy sessions are closed (so we don't hold a connection to the database)
    flask_app.do_teardown_appcontext()


def sync_with_airflow() -> None:
    """
    Preform a sync of all objects with airflow (note, `sync_with_airflow()` is called in `main()` template).
    """
    sync_all_users(user_wrappers=VAR__USER_WRAPPERS)


##############
## Run Main ##
##############
main(sync_forever=True)"
---
# Source: airflow/templates/rbac/airflow-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: airflow
  namespace: egov
  labels:
    app: airflow
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - "create"
  - "get"
  - "delete"
  - "list"
  - "patch"
  - "watch"
- apiGroups:
  - ""
  resources:
  - "pods/log"
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - "pods/exec"
  verbs:
  - "create"
  - "get"
---
# Source: airflow/templates/rbac/airflow-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: airflow
  namespace: egov
  labels:
    app: airflow
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: airflow
subjects:
- kind: ServiceAccount
  name: airflow
  namespace: egov
---
# Source: airflow/templates/flower/flower-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-flower
  namespace: egov
  labels:
    app: airflow
    component: flower
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: flower
  ports:
    - name: flower
      protocol: TCP
      port: 5555
      targetPort: 5555
---
# Source: airflow/templates/pgbouncer/pgbouncer-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-pgbouncer
  namespace: egov
  labels:
    app: airflow
    component: pgbouncer
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: pgbouncer
  ports:
    - name: pgbouncer
      protocol: TCP
      port: 6432
---
# Source: airflow/templates/webserver/webserver-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-web
  namespace: egov
  labels:
    app: airflow
    component: web
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: web
  sessionAffinity: None
  ports:
    - name: web
      protocol: TCP
      port: 8080
      targetPort: 8080
---
# Source: airflow/templates/worker/worker-service.yaml
apiVersion: v1
## this Service gives stable DNS entries for workers, used by webserver for logs
kind: Service
metadata:
  name: airflow-worker
  namespace: egov
  labels:
    app: airflow
    component: worker
spec:
  ports:
    - name: worker
      protocol: TCP
      port: 8793
  clusterIP: None
  selector:
    app: airflow
    component: worker
---
# Source: airflow/templates/db-migrations/db-migrations-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-db-migrations
  namespace: egov
  labels:
    app: airflow
    component: db-migrations
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: db-migrations
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: db-migrations
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: db-migrations          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/db_migrations.py"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
        - name: scripts
          secret:
            secretName: airflow-db-migrations
---
# Source: airflow/templates/flower/flower-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-flower
  namespace: egov
  labels:
    app: airflow
    component: flower
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple flower pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: flower
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: flower
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-flower          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          ports:
            - name: flower
              containerPort: 5555
              protocol: TCP
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow celery flower"
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            exec:
              command:
                - "bash"
                - "-c"
                - "exec curl 'http://localhost:5555'"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            exec:
              command:
                - "bash"
                - "-c"
                - "exec curl 'http://localhost:5555'"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
---
# Source: airflow/templates/pgbouncer/pgbouncer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-pgbouncer
  namespace: egov
  labels:
    app: airflow
    component: pgbouncer
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      ## multiple pgbouncer pods can safely run concurrently
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: pgbouncer
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: pgbouncer
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      terminationGracePeriodSeconds: 120
      serviceAccountName: airflow
      containers:
        - name: pgbouncer
          image: ghcr.io/airflow-helm/pgbouncer:1.17.0-patch.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 1001
            runAsGroup: 1001
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          ports:
            - name: pgbouncer
              containerPort: 6432
              protocol: TCP
          command:
            - "/usr/bin/dumb-init"
            ## rewrite SIGTERM as SIGINT, so pgbouncer does a safe shutdown
            - "--rewrite=15:2"
            - "--"
          args:
            - "/bin/sh"
            - "-c"
            ## we generate users.txt on startup, because DATABASE_PASSWORD is defined from a Secret,
            ## and we want to pickup the new values on container restart (possibly due to livenessProbe failure)
            - |-
              /home/pgbouncer/config/gen_auth_file.sh && \
              exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 60
            failureThreshold: 3
            exec:
              command:
                - "/bin/sh"
                - "-c"
                ## this check is intended to fail when the DATABASE_PASSWORD secret is updated,
                ## which would cause `gen_auth_file.sh` to run again on container start
                - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;" | grep -q "1"
          startupProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 15
            failureThreshold: 30
            tcpSocket:
              port: 6432
          volumeMounts:
            - name: pgbouncer-config
              mountPath: /home/pgbouncer/config
              readOnly: true
            - name: pgbouncer-certs
              mountPath: /home/pgbouncer/certs
              readOnly: true
      volumes:
        - name: pgbouncer-config
          secret:
            secretName: airflow-pgbouncer
            items:
              - key: gen_auth_file.sh
                path: gen_auth_file.sh
                mode: 0755
              - key: pgbouncer.ini
                path: pgbouncer.ini
        - name: pgbouncer-certs
          projected:
            sources:
              ## CLIENT TLS FILES (CHART GENERATED)
              - secret:
                  name: airflow-pgbouncer-certs
                  items:
                    - key: client.key
                      path: client.key
                    - key: client.crt
                      path: client.crt
---
# Source: airflow/templates/scheduler/scheduler-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  namespace: egov
  labels:
    app: airflow
    component: scheduler
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple schedulers can run concurrently (Airflow 2.0)
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: scheduler
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: scheduler
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-scheduler          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow scheduler -n -1"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 30
            failureThreshold: 5
            timeoutSeconds: 60
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys

                  # suppress logs triggered from importing airflow packages
                  os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                  from airflow.jobs.scheduler_job import SchedulerJob
                  from airflow.utils.db import create_session
                  from airflow.utils.net import get_hostname

                  with create_session() as session:
                      # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                      hostname = get_hostname()
                      scheduler_job = session \
                          .query(SchedulerJob) \
                          .filter_by(hostname=hostname) \
                          .order_by(SchedulerJob.latest_heartbeat.desc()) \
                          .limit(1) \
                          .first()
                      if (scheduler_job is not None) and scheduler_job.is_alive():
                          pass
                      else:
                          sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: log-cleanup  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: LOG_PATH
              value: "/opt/airflow/logs"
            - name: RETENTION_MINUTES
              value: "21600"
            - name: INTERVAL_SECONDS
              value: "900"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - |
              set -euo pipefail
        
              # break the infinite loop when we receive SIGINT or SIGTERM
              trap "exit 0" SIGINT SIGTERM
        
              while true; do
                START_EPOCH=$(date --utc +%s)
                echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."
        
                # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
                # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
                DELETED_COUNT=$(
                  find "$LOG_PATH" \
                    -type f \
                    -name "*.log" \
                    -mmin +"$RETENTION_MINUTES" \
                    -writable \
                    -delete \
                    -printf "." \
                  | wc -c
                )
        
                END_EPOCH=$(date --utc +%s)
                LOOP_DURATION=$((END_EPOCH - START_EPOCH))
                echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"
        
                SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
                if (( SECONDS_TO_SLEEP > 0 )); then
                  echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                  sleep $SECONDS_TO_SLEEP
                fi
              done
          volumeMounts:
            - name: logs-data
              mountPath: /opt/airflow/logs
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
---
# Source: airflow/templates/sync/sync-users-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-sync-users
  namespace: egov
  labels:
    app: airflow
    component: sync-users
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: sync-users
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: sync-users
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: sync-airflow-users          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/sync_users.py"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
        - name: scripts
          secret:
            secretName: airflow-sync-users
---
# Source: airflow/templates/webserver/webserver-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-web
  namespace: egov
  labels:
    app: airflow
    component: web
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple web pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: web
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: web
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      serviceAccountName: airflow
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-web          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          ports:
            - name: web
              containerPort: 8080
              protocol: TCP
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow webserver"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /health
              port: web
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /health
              port: web
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: webserver-config
              mountPath: /opt/airflow/webserver_config.py
              subPath: webserver_config.py
              readOnly: true        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
        - name: webserver-config
          secret:
            secretName: airflow-webserver-config
            defaultMode: 0644
---
# Source: airflow/templates/worker/worker-statefulset.yaml
apiVersion: apps/v1
## StatefulSet gives workers consistent DNS names, allowing webserver access to log files
kind: StatefulSet
metadata:
  name: airflow-worker
  namespace: egov
  labels:
    app: airflow
    component: worker
spec:
  serviceName: "airflow-worker"
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  ## we do not need to guarantee the order in which workers are scaled
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: airflow
      component: worker
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: worker
    spec:
      restartPolicy: Always
      terminationGracePeriodSeconds: 60
      serviceAccountName: airflow
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-worker          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
            # have dumb-init only send signals to direct child process (needed for celery workers to warm shutdown)
            - name: DUMB_INIT_SETSID
              value: "0"
          ports:
            - name: wlog
              containerPort: 8793
              protocol: TCP
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow celery worker"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: log-cleanup  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: LOG_PATH
              value: "/opt/airflow/logs"
            - name: RETENTION_MINUTES
              value: "21600"
            - name: INTERVAL_SECONDS
              value: "900"    
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_user
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: db_password    
            - name: REDIS_HOST
              value: "redis.backbone"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DBNUM
              value: "1"
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - |
              set -euo pipefail
        
              # break the infinite loop when we receive SIGINT or SIGTERM
              trap "exit 0" SIGINT SIGTERM
        
              while true; do
                START_EPOCH=$(date --utc +%s)
                echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."
        
                # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
                # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
                DELETED_COUNT=$(
                  find "$LOG_PATH" \
                    -type f \
                    -name "*.log" \
                    -mmin +"$RETENTION_MINUTES" \
                    -writable \
                    -delete \
                    -printf "." \
                  | wc -c
                )
        
                END_EPOCH=$(date --utc +%s)
                LOOP_DURATION=$((END_EPOCH - START_EPOCH))
                echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"
        
                SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
                if (( SECONDS_TO_SLEEP > 0 )); then
                  echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                  sleep $SECONDS_TO_SLEEP
                fi
              done
          volumeMounts:
            - name: logs-data
              mountPath: /opt/airflow/logs
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
---
# Source: airflow/templates/flower/flower-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: airflow-flower
  namespace: egov
  labels:
    app: airflow
    component: flower
spec:
  rules:
    - host: 
      http:
        paths:
          - path: 
            pathType: ImplementationSpecific
            backend:
              service:
                name: airflow-flower
                port:
                  name: flower
---
# Source: airflow/templates/webserver/webserver-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: airflow-web
  namespace: egov
  labels:
    app: airflow
    component: web
spec:
  rules:
    - host: 
      http:
        paths:
          - path: 
            pathType: ImplementationSpecific
            backend:
              service:
                name: airflow-web
                port:
                  name: web
