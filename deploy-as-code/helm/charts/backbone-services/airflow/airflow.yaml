---
# Source: airflow/templates/rbac/airflow-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: airflow
  labels:
    app: airflow
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
---
# Source: airflow/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.6.4
    release: "airflow"
    heritage: "Helm"
type: Opaque
data:
  postgresql-password: "YWlyZmxvdw=="
---
# Source: airflow/charts/redis/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-redis
  labels:
    app: redis
    chart: redis-10.5.7
    release: "airflow"
    heritage: "Helm"
type: Opaque
data:
  redis-password: "YWlyZmxvdw=="
---
# Source: airflow/templates/config/secret-config-envs.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-config-envs
  labels:
    app: airflow
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
## we must use `data` rather than `stringData` (see: https://github.com/helm/helm/issues/10010)
data:
  ## ================
  ## Linux Configs
  ## ================
  TZ: "RXRjL1VUQw=="

  ## ================
  ## Database Configs
  ## ================
  ## database host/port
  DATABASE_HOST: "YWlyZmxvdy1wZ2JvdW5jZXIuZGVmYXVsdC5zdmMuY2x1c3Rlci5sb2NhbA=="
  DATABASE_PORT: "NjQzMg=="

  ## database configs
  DATABASE_DB: "YWlyZmxvdw=="

  ## bash command which echos the URL encoded value of $DATABASE_USER
  DATABASE_USER_CMD: "ZWNobyAiJHtEQVRBQkFTRV9VU0VSfSIgfCBweXRob24zIC1jICJpbXBvcnQgdXJsbGliLnBhcnNlOyBlbmNvZGVkX3VzZXIgPSB1cmxsaWIucGFyc2UucXVvdGUoaW5wdXQoKSk7IHByaW50KGVuY29kZWRfdXNlciki"

  ## bash command which echos the URL encoded value of $DATABASE_PASSWORD
  DATABASE_PASSWORD_CMD: "ZWNobyAiJHtEQVRBQkFTRV9QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChlbmNvZGVkX3Bhc3MpIg=="

  ## bash command which echos the DB connection string in SQLAlchemy format
  DATABASE_SQLALCHEMY_CMD: "ZWNobyAtbiAicG9zdGdyZXNxbCtwc3ljb3BnMjovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0i"

  ## bash command which echos the DB connection string in Celery result_backend format
  DATABASE_CELERY_CMD: "ZWNobyAtbiAiZGIrcG9zdGdyZXNxbDovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0i"

  ## bash command which echos the DB connection string in `psql` cli format
  DATABASE_PSQL_CMD: "ZWNobyAtbiAicG9zdGdyZXNxbDovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0ke0RBVEFCQVNFX1BST1BFUlRJRVN9Ig=="

  ## ================
  ## Redis Configs
  ## ================
  ## connection string components
  REDIS_HOST: "YWlyZmxvdy1yZWRpcy1tYXN0ZXIuZGVmYXVsdC5zdmMuY2x1c3Rlci5sb2NhbA=="
  REDIS_PORT: "NjM3OQ=="
  REDIS_DBNUM: "MQ=="

  ## bash command which echos the URL encoded value of $REDIS_PASSWORD
  ## NOTE: if $REDIS_PASSWORD is non-empty, prints `:${REDIS_PASSWORD}@`, else ``
  REDIS_PASSWORD_CMD: "ZWNobyAiJHtSRURJU19QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChmXCI6e2VuY29kZWRfcGFzc31AXCIpIGlmIGxlbihlbmNvZGVkX3Bhc3MpID4gMCBlbHNlIE5vbmUi"

  ## bash command which echos the Redis connection string
  REDIS_CONNECTION_CMD: "ZWNobyAtbiAicmVkaXM6Ly8kKGV2YWwgJFJFRElTX1BBU1NXT1JEX0NNRCkke1JFRElTX0hPU1R9OiR7UkVESVNfUE9SVH0vJHtSRURJU19EQk5VTX0ke1JFRElTX1BST1BFUlRJRVN9Ig=="

  ## ================
  ## Airflow Configs (General)
  ## ================
  AIRFLOW__CORE__DAGS_FOLDER: "L29wdC9haXJmbG93L2RhZ3MvcmVwby9lZ292LW5hdGlvbmFsLWRhc2hib2FyZC1hY2NlbGVyYXRvci9kYWdz"
  AIRFLOW__CORE__EXECUTOR: "Q2VsZXJ5RXhlY3V0b3I="
  AIRFLOW__CORE__FERNET_KEY: "N1Q1MTJVWFNTbUJPa3BXaW1GSElWYjhqSzZsZm1TQXZ4NG1PNkFyZWhuYz0="
  AIRFLOW__CORE__SQL_ALCHEMY_CONN_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX1NRTEFMQ0hFTVlfQ01EIic="
  AIRFLOW__WEBSERVER__SECRET_KEY: "VEhJUyBJUyBVTlNBRkUh"
  AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "ODA4MA=="
  AIRFLOW__CELERY__FLOWER_PORT: "NTU1NQ=="
  ## refresh the dags folder at the same frequency as git-sync
  AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "NjA="

  ## ================
  ## Airflow Configs (Logging)
  ## ================
  AIRFLOW__LOGGING__BASE_LOG_FOLDER: "L29wdC9haXJmbG93L2xvZ3M="
  AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION: "L29wdC9haXJmbG93L2xvZ3MvZGFnX3Byb2Nlc3Nvcl9tYW5hZ2VyL2RhZ19wcm9jZXNzb3JfbWFuYWdlci5sb2c="
  AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY: "L29wdC9haXJmbG93L2xvZ3Mvc2NoZWR1bGVy"

  ## ================
  ## Airflow Configs (Celery)
  ## ================
  AIRFLOW__CELERY__WORKER_LOG_SERVER_PORT: "ODc5Mw=="
  AIRFLOW__CELERY__BROKER_URL_CMD: "YmFzaCAtYyAnZXZhbCAiJFJFRElTX0NPTk5FQ1RJT05fQ01EIic="
  AIRFLOW__CELERY__RESULT_BACKEND_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX0NFTEVSWV9DTUQiJw=="

  ## ================
  ## Airflow Configs (Kubernetes)
  ## ================

  ## ================
  ## User Configs
  ## ================
---
# Source: airflow/templates/config/secret-webserver-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-webserver-config
  labels:
    app: airflow
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
data:
  webserver_config.py: "ZnJvbSBhaXJmbG93IGltcG9ydCBjb25maWd1cmF0aW9uIGFzIGNvbmYKZnJvbSBmbGFza19hcHBidWlsZGVyLnNlY3VyaXR5Lm1hbmFnZXIgaW1wb3J0IEFVVEhfREIKCiMgdGhlIFNRTEFsY2hlbXkgY29ubmVjdGlvbiBzdHJpbmcKU1FMQUxDSEVNWV9EQVRBQkFTRV9VUkkgPSBjb25mLmdldCgnY29yZScsICdTUUxfQUxDSEVNWV9DT05OJykKCiMgdXNlIGVtYmVkZGVkIERCIGZvciBhdXRoCkFVVEhfVFlQRSA9IEFVVEhfREIK"
---
# Source: airflow/templates/db-migrations/db-migrations-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-db-migrations
  labels:
    app: airflow
    component: db-migrations
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
data:
  db_migrations.py: "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"
---
# Source: airflow/templates/pgbouncer/pgbouncer-secret-certs.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-pgbouncer-certs
  labels:
    app: airflow
    component: pgbouncer
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
data:
  client.key: "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"
  client.crt: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4VENDQWRtZ0F3SUJBZ0lRY1JpVHQ0dm50K2ZSV25FNzlwYzY3akFOQmdrcWhraUc5dzBCQVFzRkFEQVUKTVJJd0VBWURWUVFERXdsc2IyTmhiR2h2YzNRd0hoY05Nakl3TkRBME1URXlOakV4V2hjTk1qTXdOREEwTVRFeQpOakV4V2pBVU1SSXdFQVlEVlFRREV3bHNiMk5oYkdodmMzUXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRQzRFZlYzSEE0WGZvVGhPOWYrLzBLMGRIM1YzczFlZ0p5U0lCWnpoZmY3dWdYWWJWcmgKZ3dNNTc1QTg3aWlLaVVKeDNZVk10OURpS1NSUTZjSGpFRjVTZCttVjB0N0hKNllYN0FKQUlFelpta1liUGpKYwptc2IzUGxjN3hwQlM4TkZCTkkyVXZRUFlRZS9Ocy9URnlMd242OXQwMWhFZllJY2puQVpic09Rc05JcVkybFRRCmdDSlE0UUZDOVhselRJVTBJTUR4NGhqUlRMK3FxU1Z2VkZYd2MzWUNyajQxcFlHYldLMFJtYmN6S0JyaXpRRG4KU2lJK2tOS05STGJQVVM0UWt6enRiUlZNSlJuR1F3WTRkT3ZGQkhIdnR1UWhkMHRZTlFzQjU2QmxjMTVGMDR1cwpKQUo5Yy9lMTM1SEJ2Nm1LckpnSzNkZDE4dm9hTTExb3h1MHpBZ01CQUFHalB6QTlNQTRHQTFVZER3RUIvd1FFCkF3SUZvREFkQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0RBWURWUjBUQVFIL0JBSXcKQURBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWllbEUxVjJ3d2pvdnQ2V2RCSnJUblhhaXJpQ0hZTnhsSXpoaApCN0JjK3liaHhJbU1iUU96Z2Z4L3hETjNmcVUzcFIvN2xvMFJjc2FlMmkvRStZY1pZc25nYXd6QnZ2R2hqdXdYCmZVMHl4WC9PYWV0eHlWMThwTVhCUW5HNnBuTHF0N1dVWTBmeEtWQWtNaGsrY29qVnZyQUt2NXRrQW5VU2FqS1AKMHZGakhLWEpDODVoaytxS2g5WGJaU1V4VXNrMWw5dU5yRlBsVXQ3L0ZvTGJhbUtmUzV2TXIrNlJkMkdKb2VXRQpoS3ArZ2M1SXRFaGdXWjhzWUhabGJoWWhIbmxCNVVsVmJqb1FmVHFvZG9oc0JjMkV4MUxzV0x4VFV3Ym14TGxVCnBLMlZCUVByV2JFeHMzelR2ekVoNGg3dW0yVW5YeXZhRVdlRHRpWjF6QnZya1FWMGJnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo="
---
# Source: airflow/templates/pgbouncer/pgbouncer-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-pgbouncer
  labels:
    app: airflow
    component: pgbouncer
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
data:
  pgbouncer.ini: "CltkYXRhYmFzZXNdCiogPSBob3N0PWFpcmZsb3ctcG9zdGdyZXNxbC5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIHBvcnQ9NTQzMgoKW3BnYm91bmNlcl0KcG9vbF9tb2RlID0gdHJhbnNhY3Rpb24KbWF4X2NsaWVudF9jb25uID0gMTAwMApkZWZhdWx0X3Bvb2xfc2l6ZSA9ICAyMAppZ25vcmVfc3RhcnR1cF9wYXJhbWV0ZXJzID0gZXh0cmFfZmxvYXRfZGlnaXRzCgpsaXN0ZW5fcG9ydCA9IDY0MzIKbGlzdGVuX2FkZHIgPSAqCgphdXRoX3R5cGUgPSBtZDUKYXV0aF9maWxlID0gL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dAoKbG9nX2Rpc2Nvbm5lY3Rpb25zID0gMApsb2dfY29ubmVjdGlvbnMgPSAwCgojIGxvY2tzIHdpbGwgbmV2ZXIgYmUgcmVsZWFzZWQgd2hlbiBgcG9vbF9tb2RlPXRyYW5zYWN0aW9uYCAoYWlyZmxvdyBpbml0ZGIvdXBncmFkZWRiIHNjcmlwdHMgY3JlYXRlIGxvY2tzKQpzZXJ2ZXJfcmVzZXRfcXVlcnkgPSBTRUxFQ1QgcGdfYWR2aXNvcnlfdW5sb2NrX2FsbCgpCnNlcnZlcl9yZXNldF9xdWVyeV9hbHdheXMgPSAxCgojIyBDTElFTlQgVExTIFNFVFRJTkdTICMjCmNsaWVudF90bHNfc3NsbW9kZSA9IHByZWZlcgpjbGllbnRfdGxzX2NpcGhlcnMgPSBub3JtYWwKY2xpZW50X3Rsc19rZXlfZmlsZSA9IC9ob21lL3BnYm91bmNlci9jZXJ0cy9jbGllbnQua2V5CmNsaWVudF90bHNfY2VydF9maWxlID0gL2hvbWUvcGdib3VuY2VyL2NlcnRzL2NsaWVudC5jcnQKCiMjIFNFUlZFUiBUTFMgU0VUVElOR1MgIyMKc2VydmVyX3Rsc19zc2xtb2RlID0gcHJlZmVyCnNlcnZlcl90bHNfY2lwaGVycyA9IG5vcm1hbA=="
  gen_auth_file.sh: "CiMhL2Jpbi9zaCAtZQoKIyBERVNDUklQVElPTjoKIyAtIHVwZGF0ZXMgdGhlIHBnYm91bmNlciBgYXV0aF9maWxlYCBmcm9tIGVudmlyb25tZW50IHZhcmlhYmxlcwojIC0gY2FsbGVkIGluIG1haW4gcGdib3VuY2VyIGNvbnRhaW5lciBzdGFydC1jb21tYW5kIHNvIHRoYXQgYGF1dGhfZmlsZWAgaXMgdXBkYXRlZCBlYWNoIHJlc3RhcnQsCiMgICBmb3IgZXhhbXBsZSwgd2hlbiB0aGUgbGl2ZW5lc3NQcm9iZSBmYWlscyBkdWUgdG8gYSBEQVRBQkFTRV9QQVNTV09SRCBzZWNyZXQgdXBkYXRlCgojIHZhcmlhYmxlcyB0byBpbmNyZWFzZSBjbGFyaXR5IG9mIHBhdHRlcm4gbWF0Y2hpbmcKT05FX1FVT1RFPSciJwpUV09fUVVPVEU9JyIiJwoKIyBwZ2JvdW5jZXIgcmVxdWlyZXMgYCJgIHRvIGJlIGVzY2FwZWQgYXMgYCIiYApFU0NBUEVEX0RBVEFCQVNFX1VTRVI9IiR7REFUQUJBU0VfVVNFUi8kT05FX1FVT1RFLyRUV09fUVVPVEV9IgpFU0NBUEVEX0RBVEFCQVNFX1BBU1NXT1JEPSIke0RBVEFCQVNFX1BBU1NXT1JELyRPTkVfUVVPVEUvJFRXT19RVU9URX0iCgojIHBnYm91bmNlciByZXF1aXJlcyBhdXRoX2ZpbGUgaW4gZm9ybWF0IGAibXktdXNlcm5hbWUiICJteS1wYXNzd29yZCJgCmVjaG8gXCIkRVNDQVBFRF9EQVRBQkFTRV9VU0VSXCIgXCIkRVNDQVBFRF9EQVRBQkFTRV9QQVNTV09SRFwiID4gL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dAplY2hvICJTdWNjZXNzZnVsbHkgZ2VuZXJhdGVkIGF1dGhfZmlsZTogL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dCI="
---
# Source: airflow/templates/sync/sync-users-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-sync-users
  labels:
    app: airflow
    component: sync-users
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
data:
  sync_users.py: "
############################
#### BEGIN: GLOBAL CODE ####
############################
####################
## Global Imports ##
####################
import logging
import os
import time
from string import Template
from typing import List, Dict, Optional


####################
## Global Configs ##
####################
# the path which Secret/ConfigMap are mounted to
CONF__TEMPLATES_PATH = "/mnt/templates"

# how frequently to check for Secret/ConfigMap updates
CONF__TEMPLATES_SYNC_INTERVAL = 10

# how frequently to re-sync objects (Connections, Pools, Users, Variables)
CONF__OBJECTS_SYNC_INTERVAL = 60


######################
## Global Functions ##
######################
def string_substitution(raw_string: Optional[str], substitution_map: Dict[str, str]) -> str:
    """
    Apply bash-like substitutions to a raw string.

    Example:
    - string_substitution("Hello!", None) -> "Hello!"
    - string_substitution("Hello ${NAME}!", {"NAME": "Airflow"}) -> "Hello Airflow!"
    """
    if raw_string and len(substitution_map) > 0:
        tpl = Template(raw_string)
        return tpl.safe_substitute(substitution_map)
    else:
        return raw_string


def template_mtime(template_name: str) -> float:
    """
    Return the modification-time of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    return os.stat(file_path).st_mtime


def template_value(template_name: str) -> str:
    """
    Return the contents of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    with open(file_path, "r") as f:
        return f.read()


def refresh_template_cache(template_names: List[str],
                           template_mtime_cache: Dict[str, float],
                           template_value_cache: Dict[str, str]) -> List[str]:
    """
    Refresh the provided dictionary caches of template values & mtimes.

    :param template_names: the names of all templates to refresh
    :param template_mtime_cache: the dictionary cache of template file modification-times
    :param template_value_cache: the dictionary cache of template values
    :return: the names of templates which changed
    """
    changed_templates = []
    for template_name in template_names:
        old_mtime = template_mtime_cache.get(template_name, None)
        new_mtime = template_mtime(template_name)
        # first, check if the files were modified
        if old_mtime != new_mtime:
            old_value = template_value_cache.get(template_name, None)
            new_value = template_value(template_name)
            # second, check if the value actually changed
            if old_value != new_value:
                template_value_cache[template_name] = new_value
                changed_templates += [template_name]
            template_mtime_cache[template_name] = new_mtime
    return changed_templates


def main(sync_forever: bool):
    # initial sync of template cache
    refresh_template_cache(
        template_names=VAR__TEMPLATE_NAMES,
        template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
        template_value_cache=VAR__TEMPLATE_VALUE_CACHE
    )

    # initial sync of objects into Airflow DB
    sync_with_airflow()

    if sync_forever:
        # define variables used to track how long since last refresh/sync
        templates_sync_epoch = time.time()
        objects_sync_epoch = time.time()

        # main loop
        while True:
            # monitor for template secret/configmap updates
            if (time.time() - templates_sync_epoch) > CONF__TEMPLATES_SYNC_INTERVAL:
                logging.debug(f"template sync interval reached, re-syncing all templates...")
                changed_templates = refresh_template_cache(
                    template_names=VAR__TEMPLATE_NAMES,
                    template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
                    template_value_cache=VAR__TEMPLATE_VALUE_CACHE
                )
                templates_sync_epoch = time.time()
                if changed_templates:
                    logging.info(f"template values have changed: [{','.join(changed_templates)}]")
                    sync_with_airflow()
                    objects_sync_epoch = time.time()

            # monitor for external changes to objects (like from UI)
            if (time.time() - objects_sync_epoch) > CONF__OBJECTS_SYNC_INTERVAL:
                logging.debug(f"sync interval reached, re-syncing all objects...")
                sync_with_airflow()
                objects_sync_epoch = time.time()

            # ensure we dont loop too fast
            time.sleep(0.5)
##########################
#### END: GLOBAL CODE ####
##########################


#############
## Imports ##
#############
import sys
from flask_appbuilder.security.sqla.models import User, Role
from werkzeug.security import check_password_hash, generate_password_hash
import airflow.www.app as www_app
flask_app = www_app.create_app()
flask_appbuilder = flask_app.appbuilder


#############
## Classes ##
#############
class UserWrapper(object):
    def __init__(
            self,
            username: str,
            first_name: Optional[str] = None,
            last_name: Optional[str] = None,
            email: Optional[str] = None,
            roles: Optional[List[str]] = None,
            password: Optional[str] = None
    ):
        self.username = username
        self._first_name = first_name
        self._last_name = last_name
        self._email = email
        self.roles = roles
        self._password = password

    @property
    def first_name(self) -> str:
        return string_substitution(self._first_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def last_name(self) -> str:
        return string_substitution(self._last_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def email(self) -> str:
        return string_substitution(self._email, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def password(self) -> str:
        return string_substitution(self._password, VAR__TEMPLATE_VALUE_CACHE)

    def as_dict(self) -> Dict[str, str]:
        return {
            "username": self.username,
            "first_name": self.first_name,
            "last_name": self.last_name,
            "email": self.email,
            "roles": [find_role(role_name=role_name) for role_name in self.roles],
            "password": self.password
        }


###############
## Variables ##
###############
VAR__TEMPLATE_NAMES = [
]
VAR__TEMPLATE_MTIME_CACHE = {}
VAR__TEMPLATE_VALUE_CACHE = {}
VAR__USER_WRAPPERS = {
  "admin": UserWrapper(
    username="admin",
    first_name="admin",
    last_name="admin",
    email="admin@example.com",
    roles=[        "Admin",
    ],
    password="admin",
  ),
}


###############
## Functions ##
###############
def find_role(role_name: str) -> Role:
    """
    Get the FAB Role model associated with a `role_name`.
    """
    found_role = flask_appbuilder.sm.find_role(role_name)
    if found_role:
        return found_role
    else:
        valid_roles = flask_appbuilder.sm.get_all_roles()
        logging.error(f"Failed to find role=`{role_name}`, valid roles are: {valid_roles}")
        sys.exit(1)


def compare_role_lists(role_list_1: List[Role], role_list_2: List[Role]) -> bool:
    """
    Check if two lists of FAB Roles contain the same roles (ignores duplicates and order).
    """
    name_set_1 = set(role.name for role in role_list_1)
    name_set_2 = set(role.name for role in role_list_2)
    return name_set_1 == name_set_2



def compare_users(user_dict: Dict, user_model: User) -> bool:
    """
    Check if user info (stored in dict) is identical to a FAB User model.
    """
    return (
            user_dict["username"] == user_model.username
            and user_dict["first_name"] == user_model.first_name
            and user_dict["last_name"] == user_model.last_name
            and user_dict["email"] == user_model.email
            and compare_role_lists(user_dict["roles"], user_model.roles)
            and check_password_hash(pwhash=user_model.password, password=user_dict["password"])
    )


def sync_user(user_wrapper: UserWrapper) -> None:
    """
    Sync the User defined by a provided UserWrapper into the FAB DB.
    """
    username = user_wrapper.username
    u_new = user_wrapper.as_dict()
    u_old = flask_appbuilder.sm.find_user(username=username)

    if not u_old:
        logging.info(f"User=`{username}` is missing, adding...")
        created_user = flask_appbuilder.sm.add_user(
            username=u_new["username"],
            first_name=u_new["first_name"],
            last_name=u_new["last_name"],
            email=u_new["email"],
            # in old versions of flask_appbuilder `add_user(role=` can only add exactly one role
            # (unchecked 0 index is safe because we require at least one role using helm values validation)
            role=u_new["roles"][0],
            password=u_new["password"]
        )
        if created_user:
            # add the full list of roles (we only added the first one above)
            created_user.roles = u_new["roles"]
            logging.info(f"User=`{username}` was successfully added.")
        else:
            logging.error(f"Failed to add User=`{username}`")
            sys.exit(1)
    else:
        if compare_users(u_new, u_old):
            pass
        else:
            logging.info(f"User=`{username}` exists but has changed, updating...")
            u_old.first_name = u_new["first_name"]
            u_old.last_name = u_new["last_name"]
            u_old.email = u_new["email"]
            u_old.roles = u_new["roles"]
            u_old.password = generate_password_hash(u_new["password"])
            # strange check for False is because update_user() returns None for success
            # but in future might return the User model
            if not (flask_appbuilder.sm.update_user(u_old) is False):
                logging.info(f"User=`{username}` was successfully updated.")
            else:
                logging.error(f"Failed to update User=`{username}`")
                sys.exit(1)


def sync_all_users(user_wrappers: Dict[str, UserWrapper]) -> None:
    """
    Sync all users in provided `user_wrappers`.
    """
    logging.info("BEGIN: airflow users sync")
    for user_wrapper in user_wrappers.values():
        sync_user(user_wrapper)
    logging.info("END: airflow users sync")

    # ensures than any SQLAlchemy sessions are closed (so we don't hold a connection to the database)
    flask_app.do_teardown_appcontext()


def sync_with_airflow() -> None:
    """
    Preform a sync of all objects with airflow (note, `sync_with_airflow()` is called in `main()` template).
    """
    sync_all_users(user_wrappers=VAR__USER_WRAPPERS)


##############
## Run Main ##
##############
main(sync_forever=True)"
---
# Source: airflow/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-redis
  labels:
    app: redis
    chart: redis-10.5.7
    heritage: Helm
    release: airflow
data:
  redis.conf: |-
    # User-supplied configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  master.conf: |-
    dir /data
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
  replica.conf: |-
    dir /data
    slave-read-only yes
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
---
# Source: airflow/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-redis-health
  labels:
    app: redis
    chart: redis-10.5.7
    heritage: Helm
    release: airflow
data:
  ping_readiness_local.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_PASSWORD --no-auth-warning \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_PASSWORD --no-auth-warning \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_MASTER_PASSWORD --no-auth-warning \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_MASTER_PASSWORD --no-auth-warning \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: airflow/templates/rbac/airflow-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: airflow
  labels:
    app: airflow
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - "create"
  - "get"
  - "delete"
  - "list"
  - "patch"
  - "watch"
- apiGroups:
  - ""
  resources:
  - "pods/log"
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - "pods/exec"
  verbs:
  - "create"
  - "get"
---
# Source: airflow/templates/rbac/airflow-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: airflow
  labels:
    app: airflow
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: airflow
subjects:
- kind: ServiceAccount
  name: airflow
  namespace: default
---
# Source: airflow/charts/postgresql/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-postgresql-headless
  labels:
    app: postgresql
    chart: postgresql-8.6.4
    release: "airflow"
    heritage: "Helm"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "airflow"
---
# Source: airflow/charts/postgresql/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.6.4
    release: "airflow"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "airflow"
    role: master
---
# Source: airflow/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-redis-headless
  labels:
    app: redis
    chart: redis-10.5.7
    release: airflow
    heritage: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: airflow
---
# Source: airflow/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-redis-master
  labels:
    app: redis
    chart: redis-10.5.7
    release: airflow
    heritage: Helm
spec:
  type: ClusterIP
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: airflow
    role: master
---
# Source: airflow/templates/flower/flower-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-flower
  labels:
    app: airflow
    component: flower
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: flower
    release: airflow
  ports:
    - name: flower
      protocol: TCP
      port: 5555
      targetPort: 5555
---
# Source: airflow/templates/pgbouncer/pgbouncer-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-pgbouncer
  labels:
    app: airflow
    component: pgbouncer
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: pgbouncer
    release: airflow
  ports:
    - name: pgbouncer
      protocol: TCP
      port: 6432
---
# Source: airflow/templates/webserver/webserver-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-web
  labels:
    app: airflow
    component: web
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: web
    release: airflow
  sessionAffinity: None
  ports:
    - name: web
      protocol: TCP
      port: 8080
      targetPort: 8080
---
# Source: airflow/templates/worker/worker-service.yaml
apiVersion: v1
## this Service gives stable DNS entries for workers, used by webserver for logs
kind: Service
metadata:
  name: airflow-worker
  labels:
    app: airflow
    component: worker
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  ports:
    - name: worker
      protocol: TCP
      port: 8793
  clusterIP: None
  selector:
    app: airflow
    component: worker
    release: airflow
---
# Source: airflow/templates/db-migrations/db-migrations-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-db-migrations
  labels:
    app: airflow
    component: db-migrations
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: db-migrations
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: a89558f8c09fd047bd34db4c0767bea875f47ddd7ed1f49fab14c922fac4f652
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/db-migrations-script: 5f00610c570937a76488380602536f1a0487c0dea26e2a421a63560257180aae
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: db-migrations
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: db-migrations          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/db_migrations.py"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
        - name: scripts
          secret:
            secretName: airflow-db-migrations
---
# Source: airflow/templates/flower/flower-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-flower
  labels:
    app: airflow
    component: flower
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple flower pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: flower
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: a89558f8c09fd047bd34db4c0767bea875f47ddd7ed1f49fab14c922fac4f652
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: flower
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-flower          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          ports:
            - name: flower
              containerPort: 5555
              protocol: TCP
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow celery flower"
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            exec:
              command:
                - "bash"
                - "-c"
                - "exec curl 'http://localhost:5555'"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            exec:
              command:
                - "bash"
                - "-c"
                - "exec curl 'http://localhost:5555'"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
---
# Source: airflow/templates/pgbouncer/pgbouncer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-pgbouncer
  labels:
    app: airflow
    component: pgbouncer
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      ## multiple pgbouncer pods can safely run concurrently
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: pgbouncer
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: a89558f8c09fd047bd34db4c0767bea875f47ddd7ed1f49fab14c922fac4f652
        checksum/secret-pgbouncer: 76f363c9bff1de5f6e4a70a82c30ddee9cd7aecac4c5650e001050d18992f91c
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: pgbouncer
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      terminationGracePeriodSeconds: 120
      serviceAccountName: airflow
      containers:
        - name: pgbouncer
          image: ghcr.io/airflow-helm/pgbouncer:1.17.0-patch.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 1001
            runAsGroup: 1001
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          ports:
            - name: pgbouncer
              containerPort: 6432
              protocol: TCP
          command:
            - "/usr/bin/dumb-init"
            ## rewrite SIGTERM as SIGINT, so pgbouncer does a safe shutdown
            - "--rewrite=15:2"
            - "--"
          args:
            - "/bin/sh"
            - "-c"
            ## we generate users.txt on startup, because DATABASE_PASSWORD is defined from a Secret,
            ## and we want to pickup the new values on container restart (possibly due to livenessProbe failure)
            - |-
              /home/pgbouncer/config/gen_auth_file.sh && \
              exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 60
            failureThreshold: 3
            exec:
              command:
                - "/bin/sh"
                - "-c"
                ## this check is intended to fail when the DATABASE_PASSWORD secret is updated,
                ## which would cause `gen_auth_file.sh` to run again on container start
                - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;" | grep -q "1"
          startupProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 15
            failureThreshold: 30
            tcpSocket:
              port: 6432
          volumeMounts:
            - name: pgbouncer-config
              mountPath: /home/pgbouncer/config
              readOnly: true
            - name: pgbouncer-certs
              mountPath: /home/pgbouncer/certs
              readOnly: true
      volumes:
        - name: pgbouncer-config
          secret:
            secretName: airflow-pgbouncer
            items:
              - key: gen_auth_file.sh
                path: gen_auth_file.sh
                mode: 0755
              - key: pgbouncer.ini
                path: pgbouncer.ini
        - name: pgbouncer-certs
          projected:
            sources:
              ## CLIENT TLS FILES (CHART GENERATED)
              - secret:
                  name: airflow-pgbouncer-certs
                  items:
                    - key: client.key
                      path: client.key
                    - key: client.crt
                      path: client.crt
---
# Source: airflow/templates/scheduler/scheduler-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  labels:
    app: airflow
    component: scheduler
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple schedulers can run concurrently (Airflow 2.0)
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: scheduler
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: a89558f8c09fd047bd34db4c0767bea875f47ddd7ed1f49fab14c922fac4f652
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: scheduler
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-scheduler          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow scheduler -n -1"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 30
            failureThreshold: 5
            timeoutSeconds: 60
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys

                  # suppress logs triggered from importing airflow packages
                  os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                  from airflow.jobs.scheduler_job import SchedulerJob
                  from airflow.utils.db import create_session
                  from airflow.utils.net import get_hostname

                  with create_session() as session:
                      # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                      hostname = get_hostname()
                      scheduler_job = session \
                          .query(SchedulerJob) \
                          .filter_by(hostname=hostname) \
                          .order_by(SchedulerJob.latest_heartbeat.desc()) \
                          .limit(1) \
                          .first()
                      if (scheduler_job is not None) and scheduler_job.is_alive():
                          pass
                      else:
                          sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: log-cleanup  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: LOG_PATH
              value: "/opt/airflow/logs"
            - name: RETENTION_MINUTES
              value: "21600"
            - name: INTERVAL_SECONDS
              value: "900"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - |
              set -euo pipefail
        
              # break the infinite loop when we receive SIGINT or SIGTERM
              trap "exit 0" SIGINT SIGTERM
        
              while true; do
                START_EPOCH=$(date --utc +%s)
                echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."
        
                # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
                # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
                DELETED_COUNT=$(
                  find "$LOG_PATH" \
                    -type f \
                    -name "*.log" \
                    -mmin +"$RETENTION_MINUTES" \
                    -writable \
                    -delete \
                    -printf "." \
                  | wc -c
                )
        
                END_EPOCH=$(date --utc +%s)
                LOOP_DURATION=$((END_EPOCH - START_EPOCH))
                echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"
        
                SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
                if (( SECONDS_TO_SLEEP > 0 )); then
                  echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                  sleep $SECONDS_TO_SLEEP
                fi
              done
          volumeMounts:
            - name: logs-data
              mountPath: /opt/airflow/logs
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
---
# Source: airflow/templates/sync/sync-users-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-sync-users
  labels:
    app: airflow
    component: sync-users
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: sync-users
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: a89558f8c09fd047bd34db4c0767bea875f47ddd7ed1f49fab14c922fac4f652
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/sync-users-script: cbef1346dbaba52ce4169fb6c86cbbe8538961970d25518f6b4246b5d12d370a
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: sync-users
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow
      initContainers:
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: sync-airflow-users          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/sync_users.py"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
        ## git-sync is included so "airflow plugins" & "python packages" can be stored in the dags repo        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
        - name: scripts
          secret:
            secretName: airflow-sync-users
---
# Source: airflow/templates/webserver/webserver-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-web
  labels:
    app: airflow
    component: web
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple web pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: web
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: a89558f8c09fd047bd34db4c0767bea875f47ddd7ed1f49fab14c922fac4f652
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/config-webserver-config: 046c92cebea11e3580f8553a1ae7ef83d3b6394d0db53e3f5c869e77770bf5be
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: web
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      serviceAccountName: airflow
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-web          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          ports:
            - name: web
              containerPort: 8080
              protocol: TCP
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow webserver"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /health
              port: web
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /health
              port: web
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: webserver-config
              mountPath: /opt/airflow/webserver_config.py
              subPath: webserver_config.py
              readOnly: true        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
        - name: webserver-config
          secret:
            secretName: airflow-webserver-config
            defaultMode: 0644
---
# Source: airflow/charts/postgresql/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: airflow-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.6.4
    release: "airflow"
    heritage: "Helm"
spec:
  serviceName: airflow-postgresql-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: postgresql
      release: "airflow"
      role: master
  template:
    metadata:
      name: airflow-postgresql
      labels:
        app: postgresql
        chart: postgresql-8.6.4
        release: "airflow"
        heritage: "Helm"
        role: master
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    spec:      
      securityContext:
        fsGroup: 1001
      initContainers:
        # - name: do-something
        #   image: busybox
        #   command: ['do', 'something']
        
      containers:
        - name: airflow-postgresql
          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r9
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "airflow"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "airflow" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -d "airflow" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: airflow/charts/redis/templates/redis-master-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: airflow-redis-master
  labels:
    app: redis
    chart: redis-10.5.7
    release: airflow
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: redis
      release: airflow
      role: master
  serviceName: airflow-redis-headless
  template:
    metadata:
      labels:
        app: redis
        chart: redis-10.5.7
        release: airflow
        role: master
      annotations:
        checksum/health: a5073935c8eb985cf8f3128ba7abbc4121cef628a9a1b0924c95cf97d33323bf
        checksum/configmap: 2b82c78fd9186045e6e2b44cfbb38460310697cf2f2f175c9d8618dd4d42e1ca
        checksum/secret: 2b847ea7129d7018fe67bde0a99fe18debd3dc92ce000033e05a221b924aa82c
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    spec:      
      securityContext:
        fsGroup: 1001
      serviceAccountName: "default"
      containers:
      - name: airflow-redis
        image: "docker.io/bitnami/redis:5.0.7-debian-10-r32"
        imagePullPolicy: "IfNotPresent"
        securityContext:
          runAsUser: 1001
        command:
        - /bin/bash
        - -c
        - |
          if [[ -n $REDIS_PASSWORD_FILE ]]; then
            password_aux=`cat ${REDIS_PASSWORD_FILE}`
            export REDIS_PASSWORD=$password_aux
          fi
          if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
          fi
          if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
          fi
          ARGS=("--port" "${REDIS_PORT}")
          ARGS+=("--requirepass" "${REDIS_PASSWORD}")
          ARGS+=("--masterauth" "${REDIS_PASSWORD}")
          ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
          ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
          /run.sh ${ARGS[@]}
        env:
        - name: REDIS_REPLICATION_MODE
          value: master
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: airflow-redis
              key: redis-password
        - name: REDIS_PORT
          value: "6379"
        ports:
        - name: redis
          containerPort: 6379
        livenessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_liveness_local.sh 5
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_readiness_local.sh 5
        resources:
          {}
        volumeMounts:
        - name: health
          mountPath: /health
        - name: redis-data
          mountPath: /data
          subPath: 
        - name: config
          mountPath: /opt/bitnami/redis/mounted-etc
        - name: redis-tmp-conf
          mountPath: /opt/bitnami/redis/etc/
      volumes:
      - name: health
        configMap:
          name: airflow-redis-health
          defaultMode: 0755
      - name: config
        configMap:
          name: airflow-redis
      - name: "redis-data"
        emptyDir: {}
      - name: redis-tmp-conf
        emptyDir: {}
  updateStrategy:
    type: RollingUpdate
---
# Source: airflow/templates/worker/worker-statefulset.yaml
apiVersion: apps/v1
## StatefulSet gives workers consistent DNS names, allowing webserver access to log files
kind: StatefulSet
metadata:
  name: airflow-worker
  labels:
    app: airflow
    component: worker
    chart: airflow-8.5.3
    release: airflow
    heritage: Helm
spec:
  serviceName: "airflow-worker"
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  ## we do not need to guarantee the order in which workers are scaled
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: airflow
      component: worker
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: a89558f8c09fd047bd34db4c0767bea875f47ddd7ed1f49fab14c922fac4f652
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: worker
        release: airflow
    spec:
      restartPolicy: Always
      terminationGracePeriodSeconds: 60
      serviceAccountName: airflow
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: dags-git-clone
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ONE_TIME
              value: "true"
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: check-db  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: wait-for-db-migrations  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-worker          
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
            # have dumb-init only send signals to direct child process (needed for celery workers to warm shutdown)
            - name: DUMB_INIT_SETSID
              value: "0"
          ports:
            - name: wlog
              containerPort: 8793
              protocol: TCP
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow celery worker"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs        
        - name: dags-git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.5.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 65533
            runAsGroup: 65533
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: GIT_SYNC_ROOT
              value: "/dags"
            - name: GIT_SYNC_DEST
              value: "repo"
            - name: GIT_SYNC_REPO
              value: "https://github.com/pmidc-digit/utilities.git"
            - name: GIT_SYNC_BRANCH
              value: "develop"
            - name: GIT_SYNC_REV
              value: "HEAD"
            - name: GIT_SYNC_DEPTH
              value: "1"
            - name: GIT_SYNC_WAIT
              value: "60"
            - name: GIT_SYNC_TIMEOUT
              value: "120"
            - name: GIT_SYNC_ADD_USER
              value: "true"
            - name: GIT_SYNC_MAX_SYNC_FAILURES
              value: "0"
            - name: GIT_KNOWN_HOSTS
              value: "false"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          volumeMounts:
            - name: dags-data
              mountPath: /dags        
        - name: log-cleanup  
          image: apache/airflow:2.1.4-python3.8
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:
            - name: LOG_PATH
              value: "/opt/airflow/logs"
            - name: RETENTION_MINUTES
              value: "21600"
            - name: INTERVAL_SECONDS
              value: "900"    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - |
              set -euo pipefail
        
              # break the infinite loop when we receive SIGINT or SIGTERM
              trap "exit 0" SIGINT SIGTERM
        
              while true; do
                START_EPOCH=$(date --utc +%s)
                echo "[$(date --utc +%FT%T.%3N)] deleting log files older than $RETENTION_MINUTES minutes..."
        
                # delete all writable files ending in ".log" with modified-time older than $RETENTION_MINUTES
                # NOTE: `-printf "."` prints a "." for each deleted file, which we count the bytes of with `wc -c`
                DELETED_COUNT=$(
                  find "$LOG_PATH" \
                    -type f \
                    -name "*.log" \
                    -mmin +"$RETENTION_MINUTES" \
                    -writable \
                    -delete \
                    -printf "." \
                  | wc -c
                )
        
                END_EPOCH=$(date --utc +%s)
                LOOP_DURATION=$((END_EPOCH - START_EPOCH))
                echo "[$(date --utc +%FT%T.%3N)] deleted $DELETED_COUNT files in $LOOP_DURATION seconds"
        
                SECONDS_TO_SLEEP=$((INTERVAL_SECONDS - LOOP_DURATION))
                if (( SECONDS_TO_SLEEP > 0 )); then
                  echo "[$(date --utc +%FT%T.%3N)] waiting $SECONDS_TO_SLEEP seconds..."
                  sleep $SECONDS_TO_SLEEP
                fi
              done
          volumeMounts:
            - name: logs-data
              mountPath: /opt/airflow/logs
      volumes:        
        - name: dags-data
          emptyDir: {}
        - name: logs-data
          emptyDir: {}
